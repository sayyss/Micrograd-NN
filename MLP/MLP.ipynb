{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1e19479e-a095-4b30-ad28-9ed5499a3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cbfe5370-eb45-4541-a964-25bd7ec292f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d0cfbc63-f1f0-4ce3-9ec1-0a64c389a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev: ...\n",
      "next: e\n",
      "prev: ..e\n",
      "next: m\n",
      "prev: .em\n",
      "next: m\n",
      "prev: emm\n",
      "next: a\n",
      "prev: mma\n",
      "next: .\n",
      "prev: ...\n",
      "next: o\n",
      "prev: ..o\n",
      "next: l\n",
      "prev: .ol\n",
      "next: i\n",
      "prev: oli\n",
      "next: v\n",
      "prev: liv\n",
      "next: i\n",
      "prev: ivi\n",
      "next: a\n",
      "prev: via\n",
      "next: .\n",
      "prev: ...\n",
      "next: a\n",
      "prev: ..a\n",
      "next: v\n",
      "prev: .av\n",
      "next: a\n",
      "prev: ava\n",
      "next: .\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "for j in range(len(words[:3])):\n",
    "    if len(words) >= 3:\n",
    "        temp = '...' + words[j] + '.'\n",
    "        for i in range(len(temp)-k):\n",
    "            print(\"prev:\", temp[i:i+k])\n",
    "            print(\"next:\", temp[i+k:i+k+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e1f159ad-8b60-4e8e-8e33-a32ffddb8354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = sorted(list(set(''.join(words))))\n",
    "char = ['.'] + char\n",
    "stoi = {}\n",
    "for i in range(len(char)):\n",
    "    stoi[char[i]] = i\n",
    "itoi = {}\n",
    "for i in range(len(char)):\n",
    "    itoi[i] = char[i]\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a41dab39-d863-4763-88ab-5e3869d3cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "x,y = [], []\n",
    "k = 3\n",
    "for j in range(len(words)):\n",
    "    temp = '...' + words[j] + '.'\n",
    "    for i in range(len(temp)-k):\n",
    "        #print(\"prev:\", temp[i:i+k])\n",
    "        #print(\"next:\", temp[i+k:i+k+1])\n",
    "        x.append([stoi[char] for char in temp[i:i+k]])\n",
    "        y.append(stoi[temp[i+k:i+k+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2f733004-42ea-4fd4-b179-140ea849776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "dde10af2-91fb-4295-9a92-41a8b46abba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0]]), tensor([5]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:1],y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5f1d93e6-fc90-4f45-b523-b55ca2d9c759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C matrix\n",
    "# Feature vector length 2\n",
    "C = torch.randn((len(char),2), requires_grad=True)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "425a9588-16f9-4519-b637-9ecc9f1df3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2217, -0.9238], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457c06c0-ea76-45b0-a92f-42c6d3a3bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = F.one_hot(x, num_classes=27).float()\n",
    "y_one_hot = F.one_hot(y, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2d0c50-c203-4531-b19c-38f75d3179af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot[0],y_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e10cda-f67f-4d56-a2ac-3c1f0f40e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2217, -0.9238],\n",
       "        [ 0.2217, -0.9238],\n",
       "        [ 0.2217, -0.9238]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot[0] @ C\n",
    "# Gets approriate embedding vectors from feature vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634f0e49-0850-4a56-9bc2-7f065ba2560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0812d499-b9fd-4ef8-9179-a857a701890c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2217, -0.9238],\n",
       "          [ 0.2217, -0.9238],\n",
       "          [ 0.2217, -0.9238]],\n",
       " \n",
       "         [[ 0.2217, -0.9238],\n",
       "          [ 0.2217, -0.9238],\n",
       "          [-1.6810,  0.5802]],\n",
       " \n",
       "         [[ 0.2217, -0.9238],\n",
       "          [-1.6810,  0.5802],\n",
       "          [-1.2678, -0.0057]]], grad_fn=<SliceBackward0>),\n",
       " torch.Size([228146, 3, 2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:3], emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74a7fda1-e9df-4442-8497-99557af28827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row has 3 feature vectors(words) of length 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2eebf59b-6771-4e87-9ec1-5fadfa9cda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat feature vectors into one\n",
    "emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3662b35-39d5-48ef-8381-ea51e83f4b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2217, -0.9238,  0.2217, -0.9238,  0.2217, -0.9238],\n",
       "         [ 0.2217, -0.9238,  0.2217, -0.9238, -1.6810,  0.5802],\n",
       "         [ 0.2217, -0.9238, -1.6810,  0.5802, -1.2678, -0.0057]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " torch.Size([228146, 6]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each ith -> 3 chars\n",
    "emb[:3], emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "94930153-d7c5-4465-a424-913d6be25cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix: torch.Size([27, 2])\n",
      "Input: torch.Size([32, 6])\n",
      "Hidden layer: torch.Size([6, 100]) Bias: torch.Size([100])\n",
      "Output layer: torch.Size([100, 27]) Bias: torch.Size([27])\n",
      "parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "C = torch.randn((len(char),2), requires_grad=True)\n",
    "\n",
    "print(\"Embedding matrix:\", C.shape)\n",
    "print(\"Input:\", emb.shape)\n",
    "hidden_units = 100\n",
    "H = torch.randn(emb.shape[1], hidden_units)\n",
    "b = torch.randn(hidden_units)\n",
    "print(\"Hidden layer:\", H.shape, \"Bias:\", b.shape)\n",
    "\n",
    "output_layer = torch.randn(hidden_units, C.shape[0])\n",
    "d = torch.randn(C.shape[0]) \n",
    "print(\"Output layer:\", output_layer.shape, \"Bias:\", d.shape)\n",
    "\n",
    "parameters = [H,b,output_layer, d, C]\n",
    "print(\"parameters:\", sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "328abaf1-5087-4af6-b7fb-233a0eda05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters: p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4305d642-aacc-446a-ad0f-5a3d82dca5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, Softmax\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1ef24818-672b-45b8-b7da-869120ef2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax(dim=1)\n",
    "CEL = CrossEntropyLoss()\n",
    "epochs = 5\n",
    "optimizer = optim.SGD([H,output_layer, b,d, C], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "50df3356-c8c2-4bf6-bc1b-8c6d8cdd0845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(54.7190, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(51.3215, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(48.2863, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(45.5502, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(43.0741, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    emb = C[x]\n",
    "    emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])\n",
    "    layer_1_output = emb @ H + b # First layer\n",
    "    logits = layer_1_output @ output_layer + d # Output layer\n",
    "    loss = CEL(logits, y) # Loss calculation\n",
    "    print(\"loss:\", loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # Calculate how weights affect the loss\n",
    "    optimizer.step() # Change optimization parameters to lower loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "68de0a92-605d-4880-b17b-819dcbbbc2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Determining learning rate\n",
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "06a84145-e0ed-4f04-879e-ed42e93e29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(71.1559, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(64.5921, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(68.8280, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(69.0599, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(72.5800, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(57.4698, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(74.7057, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(55.8338, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(58.0311, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(70.0854, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(57.8272, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(50.0983, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(67.1386, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(56.0633, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(57.4089, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(72.2386, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(46.9183, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(58.1954, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(66.5722, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(54.6555, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(64.1354, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(67.4865, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(50.6729, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(69.1220, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(65.8744, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(63.0044, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(59.9601, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(63.5990, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(61.7208, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(59.2530, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(58.6622, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(58.8050, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(57.7851, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(63.3314, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(59.2375, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(54.3226, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(56.1879, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(55.0999, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(55.9752, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(49.9953, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(52.8079, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(47.6864, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(57.1445, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(48.6811, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(58.1542, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(60.3220, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(46.4988, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(53.5816, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(57.7418, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(52.9922, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(56.3631, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(51.1943, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(56.3093, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(55.8275, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(56.5709, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(44.4947, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(46.0895, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(41.8416, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(56.5955, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(57.3486, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(45.4041, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(46.0630, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(34.5204, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(51.0177, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(49.7984, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(44.5697, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(47.8072, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(50.8959, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(47.9463, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.2895, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(40.5705, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(42.7482, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(38.3552, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(42.4761, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(49.0105, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(41.9085, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.4650, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(43.7566, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(51.6789, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(51.2915, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(44.3981, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.0055, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(49.9594, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(48.6384, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(42.6301, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(43.5643, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(42.7103, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(39.4071, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(36.4180, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(44.9581, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.0336, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.7079, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(42.9386, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(43.1282, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(38.8923, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(39.8861, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.4940, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(40.5493, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(41.5121, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(46.0863, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(36.5683, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(34.3666, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(40.1970, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(46.4130, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(40.4492, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.6705, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(39.2843, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(40.9702, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(38.6036, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.3655, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.1097, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(43.2286, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.9275, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(38.3025, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.4870, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.4679, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.5329, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.2407, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(34.2791, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(35.1247, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.0567, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.3404, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.1299, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(35.7365, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.6457, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(41.4274, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.2229, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(36.8745, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.8370, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.3606, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(35.1294, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(35.7826, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.8192, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.1129, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.3326, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.1101, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.0848, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.9770, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.5894, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.5959, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.5638, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(39.0409, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.8266, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.3517, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.8994, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.1910, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.4265, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.3061, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.3569, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.8670, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.7178, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.2739, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.2887, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.4870, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.0983, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.4697, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.0279, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.0190, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.4886, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.5719, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.7615, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.1568, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.8317, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.7236, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.8864, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.0359, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.2592, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.1844, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.1837, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.5791, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.1260, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.3596, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.4240, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.7359, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.4791, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.3256, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.6002, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.8346, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.6506, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.2995, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.8704, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.8953, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.3609, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.6291, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.4997, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.4554, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.1880, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.6739, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.7572, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.5728, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.9042, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.1027, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.4798, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.0994, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.2468, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.0190, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.6069, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.2447, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.4903, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.9351, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.0803, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.7749, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.5223, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.1493, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.7498, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.0892, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.1520, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.9549, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.0384, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.6083, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.2051, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.3188, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.2750, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.0451, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.7104, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.8128, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.2545, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.5117, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.8588, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.9804, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.5597, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.4093, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.3695, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.7486, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.6264, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.9692, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.3699, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.9832, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.3103, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.4793, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.0915, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.0805, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.8887, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.2564, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.1178, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.9273, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.0376, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.5286, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.0203, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.3753, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.5989, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.4759, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.7738, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.6753, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.8960, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.0808, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.5056, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.5346, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.2638, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.8998, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.1191, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.8645, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.1357, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.3809, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.4258, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.9120, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.4044, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.3307, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.9968, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.6101, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.5502, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.4573, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.0927, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.8130, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.3648, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.7175, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.8189, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.8676, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.6328, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.9546, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.8643, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.4979, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.0802, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.9329, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.0493, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.6729, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.9090, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.1028, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.8343, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.9383, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.7989, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.8115, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3888, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.1754, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.4167, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.0708, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.6061, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.1142, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.0761, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.4800, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.7148, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.6913, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.7595, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.2620, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.9031, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.8442, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.1636, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.8280, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.6855, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.3276, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.1054, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.2764, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.5466, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.8639, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.2340, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.1966, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.1898, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.0438, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.5333, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.2970, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.7487, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.6227, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.6137, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.6420, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.0882, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.7925, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.1860, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.6862, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.0005, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.2559, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.8056, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3718, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.6785, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.5580, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3074, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.2250, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.0404, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.4210, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.6222, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.5492, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.3817, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.7484, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.3355, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.4990, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.2575, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.2243, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.9117, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.4922, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3308, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.0315, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9340, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.8295, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.3600, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9910, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.5225, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0471, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.2331, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7991, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4390, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.2301, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.2429, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.8757, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4485, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9322, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7936, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.7728, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9013, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0942, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7388, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.3697, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.4589, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.2530, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2606, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.2511, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4293, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.8515, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4612, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1528, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3715, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0106, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0266, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6707, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2665, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.8505, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.9473, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0162, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2477, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.7921, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6337, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5705, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5538, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4796, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4577, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9328, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.7255, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0307, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6278, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.7663, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2325, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7466, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1894, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6208, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4784, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2270, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1733, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0156, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3132, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6526, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0342, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.8146, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6486, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6962, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5694, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3925, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6815, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1507, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4123, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2231, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5208, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1873, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8506, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0549, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4622, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5427, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0144, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2834, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2654, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0177, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5788, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1138, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.3986, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4375, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6146, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7046, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2039, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5689, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0103, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2602, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1260, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4355, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4136, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1093, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8883, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0177, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9751, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0201, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7229, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8698, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3683, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0983, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4035, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4051, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9546, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8615, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3632, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3368, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0977, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2107, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8358, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1060, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0662, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5285, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4833, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5532, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6388, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9054, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5859, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6044, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2730, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6468, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4265, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1874, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2087, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4782, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1882, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1638, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8272, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3503, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6242, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7390, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7096, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1245, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5042, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7580, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5057, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.6859, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4479, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8078, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8688, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6950, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2319, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4975, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8670, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2257, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1312, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0743, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.6196, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2659, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2369, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1680, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6407, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1173, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1644, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4005, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4114, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8366, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4290, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7360, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4079, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4830, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3084, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4287, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4019, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9525, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2674, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4605, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0947, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5028, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3760, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6573, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8253, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6236, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2319, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7571, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.6956, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2576, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0870, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6131, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3861, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0250, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6446, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2558, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4716, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1182, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4521, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1875, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4369, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7639, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4017, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0951, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4755, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9544, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2064, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8680, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9403, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5073, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5895, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4972, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4295, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6608, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1048, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0829, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8768, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4200, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1898, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4139, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7864, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2204, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8474, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4335, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8403, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3053, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3159, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8782, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2546, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6741, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8763, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4988, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4859, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9162, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1613, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2979, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9189, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6109, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0819, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1554, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2360, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7641, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1682, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4793, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8673, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1383, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0928, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0289, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3887, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2359, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9092, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2306, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9459, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1098, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9416, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.9112, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1294, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4054, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4339, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2154, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7954, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0908, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9870, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2502, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8953, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2704, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0972, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7335, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4745, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3195, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9776, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2901, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0332, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1390, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0481, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2390, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5051, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2902, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0141, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9989, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3072, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7932, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5186, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3265, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5886, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9723, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0634, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1280, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1479, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7958, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3697, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4463, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8364, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3503, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1729, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0480, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4695, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0016, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2982, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2983, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8799, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6221, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5025, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3643, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3244, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7728, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2012, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3180, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6723, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5818, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5921, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0057, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2950, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3914, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5321, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7354, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2552, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7377, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3245, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3076, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8246, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3419, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4140, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7687, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0367, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9631, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7919, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0243, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9382, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6825, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6027, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5826, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3804, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0891, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1229, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7864, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6451, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2307, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4921, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3113, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4999, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5318, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3764, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0369, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3340, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1873, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1714, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5734, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.9997, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6597, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4191, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0159, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2403, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.1564, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1046, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7510, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6658, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3705, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0200, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8190, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6116, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3101, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2249, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.5717, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8502, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0111, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6652, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0900, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4384, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4831, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8605, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7558, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.1719, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.8549, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4896, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1180, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7790, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.5379, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9967, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4423, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2480, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9248, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8949, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1147, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5214, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4286, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1713, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7432, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8605, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3065, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0439, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0352, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2712, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0084, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.7651, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4834, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.9000, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4743, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0556, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.7224, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9447, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4054, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7249, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7543, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4980, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4751, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6156, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0556, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3968, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9028, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7165, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4868, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1378, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0961, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0805, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8073, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3888, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7611, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9901, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.9737, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4913, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1416, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5752, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9650, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.9868, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2265, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2052, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3886, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3865, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9898, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0484, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4244, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.6680, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4969, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1625, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.8408, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3044, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5645, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4650, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.8650, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.3738, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2249, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.7257, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.8360, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6747, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9669, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4309, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.7870, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.0648, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2423, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.8363, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.8927, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.2269, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1889, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0684, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.5720, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5729, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7860, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1939, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5875, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.9471, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.1942, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.1914, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3307, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0958, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.5532, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7537, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1265, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1567, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4849, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4569, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.4883, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.8111, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4821, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3845, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4705, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2825, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2990, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0307, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4707, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.3733, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6178, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0823, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9404, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.9094, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.3550, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.6728, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.0431, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.2910, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.1703, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.3227, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.1159, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.1005, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.3962, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.3173, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.8224, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.9919, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.8876, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.5964, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(58.1497, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(115.6328, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(191.3249, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(472.9086, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2498.6084, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17714.7168, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(140395.0469, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11443758., grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0319e+11, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4029e+20, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(nan, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run a single iteration with all different learning rates, store loss for each \n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "\n",
    "    ix = torch.randint(0, x.shape[0], (32,))\n",
    "    emb = C[x[ix]]\n",
    "    emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])\n",
    "    layer_1_output = emb @ H + b # First layer\n",
    "    logits = layer_1_output @ output_layer + d # Output layer\n",
    "    loss = CEL(logits, y[ix]) # Loss calculation\n",
    "    print(\"loss:\", loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = lrs[epoch]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5786e5ca-8155-43f4-8190-6241c5c9670d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7b62cedfc590>]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGsCAYAAAAPJKchAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlF0lEQVR4nO3df3CU9YHH8c8Skk1Usl7AJJtLkICaQjgsJp0Sh/BDaiKxVFumozeOgK06aUFac7lxgp623HSCHXQQfyRiAY/Bim0XkA4IxDEJtoargU3hKiB60WDYNYNXd4HaXQLf+8Nh65oE2JD4zS7v18wz4/Ps98l+n2ceJ292n806jDFGAAAAlgyzPQEAAHBpI0YAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVcRUju3bt0pw5c5STkyOHw6HNmzfHtH9TU5Nuu+02ud1uXX755fr617+ul156qce45uZmFRUVKTU1VWPHjlV9ff0AHQEAAPiyuIqRkydP6vrrr9czzzzTr/3feustTZo0SR6PR/v27dMPfvADzZs3T7///e8jY9rb21VRUaHS0lJ5vV4tWbJEixcvlsfjGajDAAAAX+CI1y/Kczgc2rRpk26//fbItnA4rEceeUQvvfSSPv30U02cOFGPP/64ZsyY0efPufXWW5WVlaU1a9ZIkh566CFt2bJFBw4ciIyprKzUn//8Z7W0tAzW4QAAcMmKq1dGzueee+7RH//4R23YsEH79u3T97//fd1yyy06fPhwn/sEAgFlZGRE1ltaWlRWVhY1pry8XK2trTp16tSgzR0AgEtVwsTI+++/r5dfflm//e1vVVpaqnHjxqm6ulpTp07V2rVre93nd7/7nd5++23dc889kW1+v19ZWVlR47KystTd3a1jx44N6jEAAHApGm57AgNl7969Msbouuuui9oeCoU0cuTIHuObmpq0YMECvfDCCyosLIx6zOFwRK2ffSfry9sBAMDFS5gYOXPmjJKSkrRnzx4lJSVFPXbFFVdErTc3N2vOnDl68sknNW/evKjHsrOz5ff7o7Z1dXVp+PDhvUYNAAC4OAkTI5MnT9bp06fV1dWl0tLSPsc1NTXp29/+th5//HHdf//9PR4vKSmJ+nSNJO3cuVPFxcVKTk4e8HkDAHCpi6sYOXHihN57773Ient7u9ra2pSRkaHrrrtOd911l+bNm6cnnnhCkydP1rFjx/TGG2/oX/7lX1RRUaGmpibdeuut+slPfqK5c+dGXgFJSUmJ3MRaWVmpZ555RlVVVbrvvvvU0tKi1atX6+WXX7ZyzAAAJDwTRxobG42kHsv8+fONMcaEw2Hz6KOPmjFjxpjk5GSTnZ1tvvvd75p9+/YZY4yZP39+r/tPnz496nmamprM5MmTTUpKihkzZoypq6v7io8UAIBLR9z+nREAAJAYEuajvQAAID4RIwAAwKq4uIH1zJkzOnr0qEaMGMHf+gAAIE4YY3T8+HHl5ORo2LC+X/+Iixg5evSo8vLybE8DAAD0w5EjR5Sbm9vn43ERIyNGjJD0+cGkp6dbng0AALgQwWBQeXl5kd/jfYmLGDn71kx6ejoxAgBAnDnfLRYx3cBaV1enSZMmRaKgpKREr732Wp/jm5qa5HA4eiwHDx6M5WkBAEACi+mVkdzcXC1btkzXXHONJOm//uu/dNttt8nr9fb4srkvOnToUNQrGldddVU/pwsAABJNTDEyZ86cqPVf/OIXqqur0+7du88ZI5mZmbryyiv7NUEAAJDY+v13Rk6fPq0NGzbo5MmTKikpOefYyZMny+12a9asWWpsbDzvzw6FQgoGg1ELAABITDHHyP79+3XFFVfI6XSqsrJSmzZt0oQJE3od63a7tWrVKnk8Hm3cuFEFBQWaNWuWdu3adc7nqK2tlcvliix8rBcAgMQV83fThMNhdXR06NNPP5XH49GvfvUrNTc39xkkXzZnzhw5HA5t2bKlzzGhUEihUCiyfvajQYFAgE/TAAAQJ4LBoFwu13l/f8f80d6UlJTIDazFxcV6++239dRTT+n555+/oP2nTJmi9evXn3OM0+mU0+mMdWoAACAOXfR30xhjol7FOB+v1yu3232xTwsAABJETK+MLFmyRLNnz1ZeXp6OHz+uDRs2qKmpSdu3b5ck1dTUqLOzU+vWrZMkrVixQmPGjFFhYaHC4bDWr18vj8cjj8cz8EcCAADiUkwx8vHHH+vuu++Wz+eTy+XSpEmTtH37dt18882SJJ/Pp46Ojsj4cDis6upqdXZ2Ki0tTYWFhdq6dasqKioG9igAAEDcivkGVhsu9AYYAAAwdAzaDawAMFCMMfrs1GlJUlpy0nm/vwJAYrroG1gBoL8+O3VaEx7doQmP7ohECYBLDzECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFbFFCN1dXWaNGmS0tPTlZ6erpKSEr322mvn3Ke5uVlFRUVKTU3V2LFjVV9ff1ETBgAAiSWmGMnNzdWyZcvU2tqq1tZW3XTTTbrtttv0l7/8pdfx7e3tqqioUGlpqbxer5YsWaLFixfL4/EMyOQBAED8Gx7L4Dlz5kSt/+IXv1BdXZ12796twsLCHuPr6+s1evRorVixQpI0fvx4tba2avny5Zo7d27/Zw0AABJGv+8ZOX36tDZs2KCTJ0+qpKSk1zEtLS0qKyuL2lZeXq7W1ladOnWqz58dCoUUDAajFgAAkJhijpH9+/friiuukNPpVGVlpTZt2qQJEyb0Otbv9ysrKytqW1ZWlrq7u3Xs2LE+n6O2tlYulyuy5OXlxTpNAAAQJ2KOkYKCArW1tWn37t360Y9+pPnz5+udd97pc7zD4YhaN8b0uv2LampqFAgEIsuRI0dinSYAAIgTMd0zIkkpKSm65pprJEnFxcV6++239dRTT+n555/vMTY7O1t+vz9qW1dXl4YPH66RI0f2+RxOp1NOpzPWqQEAgDh00X9nxBijUCjU62MlJSVqaGiI2rZz504VFxcrOTn5Yp8aAAAkgJhiZMmSJXrzzTf1wQcfaP/+/Xr44YfV1NSku+66S9Lnb6/MmzcvMr6yslIffvihqqqqdODAAa1Zs0arV69WdXX1wB4FAACIWzG9TfPxxx/r7rvvls/nk8vl0qRJk7R9+3bdfPPNkiSfz6eOjo7I+Pz8fG3btk0PPvignn32WeXk5GjlypV8rBcAAEQ4zNk7SoewYDAol8ulQCCg9PR029MBMED+Fu7WhEd3SJLeWVquy1Jivo0NwBB2ob+/+W4aAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWBVTjNTW1uob3/iGRowYoczMTN1+++06dOjQOfdpamqSw+HosRw8ePCiJg4AABJDTDHS3NyshQsXavfu3WpoaFB3d7fKysp08uTJ8+576NAh+Xy+yHLttdf2e9IAACBxDI9l8Pbt26PW165dq8zMTO3Zs0fTpk07576ZmZm68sorY54gAABIbBd1z0ggEJAkZWRknHfs5MmT5Xa7NWvWLDU2Np5zbCgUUjAYjFoAAEBi6neMGGNUVVWlqVOnauLEiX2Oc7vdWrVqlTwejzZu3KiCggLNmjVLu3bt6nOf2tpauVyuyJKXl9ffaQIAgCHOYYwx/dlx4cKF2rp1q/7whz8oNzc3pn3nzJkjh8OhLVu29Pp4KBRSKBSKrAeDQeXl5SkQCCg9Pb0/0wUwBP0t3K0Jj+6QJL2ztFyXpcT0zjGAIS4YDMrlcp3393e/Xhl54IEHtGXLFjU2NsYcIpI0ZcoUHT58uM/HnU6n0tPToxYAAJCYYvpniDFGDzzwgDZt2qSmpibl5+f360m9Xq/cbne/9gUAAIklphhZuHChfv3rX+vVV1/ViBEj5Pf7JUkul0tpaWmSpJqaGnV2dmrdunWSpBUrVmjMmDEqLCxUOBzW+vXr5fF45PF4BvhQAABAPIopRurq6iRJM2bMiNq+du1aLViwQJLk8/nU0dEReSwcDqu6ulqdnZ1KS0tTYWGhtm7dqoqKioubOQAASAj9voH1q3ShN8AAiC/cwAoktkG9gRUAAGCgECMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArIopRmpra/WNb3xDI0aMUGZmpm6//XYdOnTovPs1NzerqKhIqampGjt2rOrr6/s9YQAAkFhiipHm5mYtXLhQu3fvVkNDg7q7u1VWVqaTJ0/2uU97e7sqKipUWloqr9erJUuWaPHixfJ4PBc9eQAAEP+GxzJ4+/btUetr165VZmam9uzZo2nTpvW6T319vUaPHq0VK1ZIksaPH6/W1lYtX75cc+fO7d+sAQBAwrioe0YCgYAkKSMjo88xLS0tKisri9pWXl6u1tZWnTp1qtd9QqGQgsFg1AIAABJTv2PEGKOqqipNnTpVEydO7HOc3+9XVlZW1LasrCx1d3fr2LFjve5TW1srl8sVWfLy8vo7TQAAMMT1O0YWLVqkffv26eWXXz7vWIfDEbVujOl1+1k1NTUKBAKR5ciRI/2dJgAAGOJiumfkrAceeEBbtmzRrl27lJube86x2dnZ8vv9Udu6uro0fPhwjRw5std9nE6nnE5nf6YGAADiTEyvjBhjtGjRIm3cuFFvvPGG8vPzz7tPSUmJGhoaorbt3LlTxcXFSk5Ojm22AAAg4cQUIwsXLtT69ev161//WiNGjJDf75ff79dnn30WGVNTU6N58+ZF1isrK/Xhhx+qqqpKBw4c0Jo1a7R69WpVV1cP3FEAAIC4FVOM1NXVKRAIaMaMGXK73ZHllVdeiYzx+Xzq6OiIrOfn52vbtm1qamrS17/+df3nf/6nVq5cycd6AQCApBjvGTl74+m5vPjiiz22TZ8+XXv37o3lqQAAwCWC76YBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArIo5Rnbt2qU5c+YoJydHDodDmzdvPuf4pqYmORyOHsvBgwf7O2cAAJBAhse6w8mTJ3X99dfrnnvu0dy5cy94v0OHDik9PT2yftVVV8X61AAAIAHFHCOzZ8/W7NmzY36izMxMXXnllTHvBwAAEttXds/I5MmT5Xa7NWvWLDU2Np5zbCgUUjAYjFoAAEBiGvQYcbvdWrVqlTwejzZu3KiCggLNmjVLu3bt6nOf2tpauVyuyJKXlzfY0wQAAJbE/DZNrAoKClRQUBBZLykp0ZEjR7R8+XJNmzat131qampUVVUVWQ8GgwQJAAAJyspHe6dMmaLDhw/3+bjT6VR6enrUAgAAEpOVGPF6vXK73TaeGgAADDExv01z4sQJvffee5H19vZ2tbW1KSMjQ6NHj1ZNTY06Ozu1bt06SdKKFSs0ZswYFRYWKhwOa/369fJ4PPJ4PAN3FAAAIG7FHCOtra2aOXNmZP3svR3z58/Xiy++KJ/Pp46Ojsjj4XBY1dXV6uzsVFpamgoLC7V161ZVVFQMwPQBAEC8cxhjjO1JnE8wGJTL5VIgEOD+ESCB/C3crQmP7pAkvbO0XJelDPo99QC+Qhf6+5vvpgEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVMcfIrl27NGfOHOXk5MjhcGjz5s3n3ae5uVlFRUVKTU3V2LFjVV9f35+5AgCABBRzjJw8eVLXX3+9nnnmmQsa397eroqKCpWWlsrr9WrJkiVavHixPB5PzJMFAACJZ3isO8yePVuzZ8++4PH19fUaPXq0VqxYIUkaP368WltbtXz5cs2dOzfWpwcAAAlm0O8ZaWlpUVlZWdS28vJytba26tSpU73uEwqFFAwGoxYAAJCYBj1G/H6/srKyorZlZWWpu7tbx44d63Wf2tpauVyuyJKXlzfY0wQAAJZ8JZ+mcTgcUevGmF63n1VTU6NAIBBZjhw5MuhzBAAAdsR8z0issrOz5ff7o7Z1dXVp+PDhGjlyZK/7OJ1OOZ3OwZ4aAAAYAgb9lZGSkhI1NDREbdu5c6eKi4uVnJw82E8PAACGuJhj5MSJE2pra1NbW5ukzz+629bWpo6ODkmfv8Uyb968yPjKykp9+OGHqqqq0oEDB7RmzRqtXr1a1dXVA3MEAAAgrsX8Nk1ra6tmzpwZWa+qqpIkzZ8/Xy+++KJ8Pl8kTCQpPz9f27Zt04MPPqhnn31WOTk5WrlyJR/rBQAAkvoRIzNmzIjcgNqbF198sce26dOna+/evbE+FQAAuATw3TQAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVf2Kkeeee075+flKTU1VUVGR3nzzzT7HNjU1yeFw9FgOHjzY70kDAIDEEXOMvPLKK/rpT3+qhx9+WF6vV6WlpZo9e7Y6OjrOud+hQ4fk8/kiy7XXXtvvSQMAgMQRc4w8+eST+uEPf6h7771X48eP14oVK5SXl6e6urpz7peZmans7OzIkpSU1O9JAwCAxBFTjITDYe3Zs0dlZWVR28vKyvTWW2+dc9/JkyfL7XZr1qxZamxsPOfYUCikYDAYtQAAgMQUU4wcO3ZMp0+fVlZWVtT2rKws+f3+Xvdxu91atWqVPB6PNm7cqIKCAs2aNUu7du3q83lqa2vlcrkiS15eXizTBAAAcWR4f3ZyOBxR68aYHtvOKigoUEFBQWS9pKRER44c0fLlyzVt2rRe96mpqVFVVVVkPRgMEiQAACSomF4ZGTVqlJKSknq8CtLV1dXj1ZJzmTJlig4fPtzn406nU+np6VELAABITDHFSEpKioqKitTQ0BC1vaGhQTfeeOMF/xyv1yu32x3LUwMAgAQV89s0VVVVuvvuu1VcXKySkhKtWrVKHR0dqqyslPT5WyydnZ1at26dJGnFihUaM2aMCgsLFQ6HtX79enk8Hnk8noE9EgAAEJdijpE77rhDn3zyiZYuXSqfz6eJEydq27ZtuvrqqyVJPp8v6m+OhMNhVVdXq7OzU2lpaSosLNTWrVtVUVExcEcBAADilsMYY2xP4nyCwaBcLpcCgQD3jwAJ5G/hbk14dIck6Z2l5bospV/31AMYoi709zffTQMAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABY1a8Yee6555Sfn6/U1FQVFRXpzTffPOf45uZmFRUVKTU1VWPHjlV9fX2/JgsAABJPzDHyyiuv6Kc//akefvhheb1elZaWavbs2ero6Oh1fHt7uyoqKlRaWiqv16slS5Zo8eLF8ng8Fz15AAAQ/xzGGBPLDt/85jd1ww03qK6uLrJt/Pjxuv3221VbW9tj/EMPPaQtW7bowIEDkW2VlZX685//rJaWlgt6zmAwKJfLpUAgoPT09FimC2AI+1u4WxMe3SFJemdpuS5LGW55RgAG0oX+/o7plZFwOKw9e/aorKwsantZWZneeuutXvdpaWnpMb68vFytra06depUr/uEQiEFg8GoBQAAJKaYYuTYsWM6ffq0srKyorZnZWXJ7/f3uo/f7+91fHd3t44dO9brPrW1tXK5XJElLy8vlmkCAIA40q8bWB0OR9S6MabHtvON7237WTU1NQoEApHlyJEj/ZkmgCEuLTlJ7ywt1ztLy5WWnGR7OgAsiekN2lGjRikpKanHqyBdXV09Xv04Kzs7u9fxw4cP18iRI3vdx+l0yul0xjI1AHHI4XBwnwiA2F4ZSUlJUVFRkRoaGqK2NzQ06MYbb+x1n5KSkh7jd+7cqeLiYiUnJ8c4XQAAkGhifpumqqpKv/rVr7RmzRodOHBADz74oDo6OlRZWSnp87dY5s2bFxlfWVmpDz/8UFVVVTpw4IDWrFmj1atXq7q6euCOAgAAxK2YXx+944479Mknn2jp0qXy+XyaOHGitm3bpquvvlqS5PP5ov7mSH5+vrZt26YHH3xQzz77rHJycrRy5UrNnTt34I4CAADErZj/zogN/J0RAADiz6D8nREAAICBRowAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYFRdfl3n2j8QGg0HLMwEAABfq7O/t8/2x97iIkePHj0uS8vLyLM8EAADE6vjx43K5XH0+HhffTXPmzBkdPXpUI0aMkMPhsD2dISsYDCovL09HjhzhO3wuAOfrwnGuYsP5unCcq9jE2/kyxuj48ePKycnRsGF93xkSF6+MDBs2TLm5ubanETfS09Pj4iIdKjhfF45zFRvO14XjXMUmns7XuV4ROYsbWAEAgFXECAAAsIoYSSBOp1OPPfaYnE6n7anEBc7XheNcxYbzdeE4V7FJ1PMVFzewAgCAxMUrIwAAwCpiBAAAWEWMAAAAq4gRAABgFTESR77zne9o9OjRSk1Nldvt1t13362jR4+ec5+NGzeqvLxco0aNksPhUFtbW48xoVBIDzzwgEaNGqXLL79c3/nOd/TRRx8N0lF8dfpzvowx+tnPfqacnBylpaVpxowZ+stf/hI1ZsaMGXI4HFHLnXfeOZiHMugG61wl4rX1wQcf6Ic//KHy8/OVlpamcePG6bHHHlM4HD7nfh9//LEWLFignJwcXXbZZbrlllt0+PDhqDGJdm0N5rni2vqHEydOaNGiRcrNzVVaWprGjx+vurq6qDFD/toyiBtPPvmkaWlpMR988IH54x//aEpKSkxJSck591m3bp35+c9/bl544QUjyXi93h5jKisrzT//8z+bhoYGs3fvXjNz5kxz/fXXm+7u7kE6kq9Gf87XsmXLzIgRI4zH4zH79+83d9xxh3G73SYYDEbGTJ8+3dx3333G5/NFlk8//XSwD2dQDda5SsRr67XXXjMLFiwwO3bsMO+//7559dVXTWZmpvm3f/u3Pvc5c+aMmTJliiktLTV/+tOfzMGDB839999vRo8ebU6cOBEZl2jX1mCeK66tf7j33nvNuHHjTGNjo2lvbzfPP/+8SUpKMps3b46MGerXFjESx1599VXjcDhMOBw+79j29vZeY+TTTz81ycnJZsOGDZFtnZ2dZtiwYWb79u0DPWWrzne+zpw5Y7Kzs82yZcsi2/7+978bl8tl6uvrI9umT59ufvKTnwz2dK0aiHN1KV1bv/zlL01+fn6fjx86dMhIMv/zP/8T2dbd3W0yMjLMCy+8ENl2KVxbA3GuuLaiFRYWmqVLl0Ztu+GGG8wjjzwSWR/q1xZv08Sp//u//9NLL72kG2+8UcnJyf3+OXv27NGpU6dUVlYW2ZaTk6OJEyfqrbfeGoipDgkXcr7a29vl9/ujzoXT6dT06dN7nIuXXnpJo0aNUmFhoaqrqyPfLJ0IBupcXSrXliQFAgFlZGT0+XgoFJIkpaamRrYlJSUpJSVFf/jDH6LGJvK1JQ3MueLaijZ16lRt2bJFnZ2dMsaosbFR7777rsrLy6PGDeVrixiJMw899JAuv/xyjRw5Uh0dHXr11Vcv6uf5/X6lpKTon/7pn6K2Z2Vlye/3X9TPHgpiOV9njzcrKytq+5fPxV133aWXX35ZTU1N+o//+A95PB5973vfG5wD+AoN9LlK9GvrrPfff19PP/20Kisr+xzzta99TVdffbVqamr017/+VeFwWMuWLZPf75fP54uMS9Rr66yBOldcW9FWrlypCRMmKDc3VykpKbrlllv03HPPaerUqZExQ/3aIkYs+9nPftbjpqIvL62trZHx//7v/y6v16udO3cqKSlJ8+bNkxmEP6JrjJHD4Rjwn3uxvorz9eXj/vK5uO+++/Stb31LEydO1J133qnf/e53ev3117V3796BPdiLNBTOVW8S5dqSpKNHj+qWW27R97//fd177719/uzk5GR5PB69++67ysjI0GWXXaampibNnj1bSUlJkXGJem1JA3+uenMpXlvS5zGye/dubdmyRXv27NETTzyhH//4x3r99dcjY4b6tTXc9gQudYsWLTrvHc1jxoyJ/PeoUaM0atQoXXfddRo/frzy8vK0e/dulZSU9Ov5s7OzFQ6H9de//jXqXxldXV268cYb+/UzB9Ngnq/s7GxJn/+ry+12R7Z3dXX1eAXgi2644QYlJyfr8OHDuuGGG2I8osFj+1wl+rV19OhRzZw5UyUlJVq1atV5f35RUZHa2toUCAQUDod11VVX6Zvf/KaKi4v73CdRrq2BPldcW//w2WefacmSJdq0aZNuvfVWSdKkSZPU1tam5cuX61vf+lav+w25a8va3Sq4aB0dHUaSaWxsPO/Y893A+sorr0S2HT16NCFvBDvf+Tp7U+bjjz8e2RYKhXrcwPpl+/fvN5JMc3PzQE/ZmoE4V4l8bX300Ufm2muvNXfeeWe/P73x7rvvmmHDhpkdO3b0OSYRrq3BOFdcW/8QCASMJLNt27ao7ffff7+5+eab+9xvqF1bxEic+O///m/z9NNPG6/Xaz744APzxhtvmKlTp5px48aZv//975FxBQUFZuPGjZH1Tz75xHi9XrN161YjyWzYsMF4vV7j8/kiYyorK01ubq55/fXXzd69e81NN90U9x+R6+/5WrZsmXG5XGbjxo1m//795l//9V+jPq763nvvmZ///Ofm7bffNu3t7Wbr1q3ma1/7mpk8eXLcnq/BOlfGJOa11dnZaa655hpz0003mY8++ijqo5Jf9OXz9Zvf/MY0Njaa999/32zevNlcffXV5nvf+17k8US8tgbrXBnDtfXF8zV9+nRTWFhoGhsbzf/+7/+atWvXmtTUVPPcc88ZY+Lj2iJG4sS+ffvMzJkzTUZGhnE6nWbMmDGmsrLSfPTRR1HjJJm1a9dG1teuXWsk9Vgee+yxyJjPPvvMLFq0yGRkZJi0tDTz7W9/23R0dHxFRzY4+nu+zpw5Yx577DGTnZ1tnE6nmTZtmtm/f3/k8Y6ODjNt2jSTkZFhUlJSzLhx48zixYvNJ5988lUd2oAbrHNlTGJeW339P/XlF5q/fL6eeuopk5uba5KTk83o0aPNI488YkKhUOTxRLy2ButcGcO19cXz5fP5zIIFC0xOTo5JTU01BQUF5oknnjBnzpwxxsTHteUwZhDufgQAALhAfJoGAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKz6f2fU65/tU5xCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d22566-8f2b-4ef0-852d-34222cf57d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
