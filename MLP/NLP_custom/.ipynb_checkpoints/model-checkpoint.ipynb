{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e19479e-a095-4b30-ad28-9ed5499a3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfe5370-eb45-4541-a964-25bd7ec292f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"Word\"\n",
    "w2 = \"Word2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0cfbc63-f1f0-4ce3-9ec1-0a64c389a037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw1,w2 -> [[0.2,0.2,0.4]\\n         [0.1,0.2,0.4]]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "w1,w2 -> [[0.2,0.2,0.4]\n",
    "         [0.1,0.2,0.4]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f159ad-8b60-4e8e-8e33-a32ffddb8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural model\n",
    "\n",
    "# 1) Map all words from vocab to a real vector of size m\n",
    "# 2) C matrix -> (len(vocab), m)\n",
    "# 3) all items in C are trainable\n",
    "\n",
    "# Probability function\n",
    "# 1) function g(input sequence of feature vectors(words)) -> maps input sequence to a next possible \n",
    "# word using a conditional probability distribution\n",
    "# output from g -> vector whose ith element estimates probability P(w_t|w_t-1)\n",
    "# g is the neural network\n",
    "\n",
    "# Combine both g and matrix C\n",
    "# 1) function f(sequence_of_vectors(words)) -> g(i, C(w_t-1), C(w_t-n+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41dab39-d863-4763-88ab-5e3869d3cc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f733004-42ea-4fd4-b179-140ea849776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny = b + Wx + U * tanh(d + Hx)\\n\\nx = concat of all input sequence feature vectors(words)\\nb = biases for W\\nd = biases for H\\nW = direct representation matrix\\nH = hidden layer matrix\\nU = another hidden to output layer matrix\\n\\ny = (Wx + b) + (U * tanh(d+Hx))\\ny =  (1,|V|) +   (1, |V|) \\n     \\ngoes to two different models, addition = (1,|V|) + (1, |V|) = (1,|V|)\\n|V| -> length of vocabuluary\\n\\nthen (1,|V|) -> softmax -> probabilities for each word in vocab\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural model in terms of matrix\n",
    "\"\"\"\n",
    "y = b + Wx + U * tanh(d + Hx)\n",
    "\n",
    "x = concat of all input sequence feature vectors(words)\n",
    "b = biases for W\n",
    "d = biases for H\n",
    "W = direct representation matrix\n",
    "H = hidden layer matrix\n",
    "U = another hidden to output layer matrix\n",
    "\n",
    "y = (Wx + b) + (U * tanh(d+Hx))\n",
    "y =  (1,|V|) +   (1, |V|) \n",
    "     \n",
    "goes to two different models, addition = (1,|V|) + (1, |V|) = (1,|V|)\n",
    "|V| -> length of vocabuluary\n",
    "\n",
    "then (1,|V|) -> softmax -> probabilities for each word in vocab\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee48ba0-8f80-48b0-bc88-57465faf46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\*'\n",
      "/tmp/ipykernel_4719/2420428203.py:9: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  file_content = re.split('; |, |\\*|\\n', file_content)\n"
     ]
    }
   ],
   "source": [
    "# Prep dataset\n",
    "\n",
    "import re\n",
    "\n",
    "words = []\n",
    "\n",
    "with open(\"dataset.txt\",\"r\") as file:\n",
    "    file_content = file.read()\n",
    "    file_content = re.split('; |, |\\*|\\n', file_content)\n",
    "    file_content = re.split(\" \", str(file_content))\n",
    "    words.extend(list(set(file_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc67ce34-7f5c-4ffc-a10d-d07a3e9cc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words[1:] # Remove empty word \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87632bb4-a0ec-42a0-8627-ca787db35039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18988"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4457bd91-ea30-46d0-84e4-f6250915a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_i = {}\n",
    "\n",
    "for i in enumerate(words):\n",
    "    word_to_i[i[1]] = i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1e2d561-3a48-44c8-b3a5-b315ee2846ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_word = {}\n",
    "for i in enumerate(words):\n",
    "    i_to_word[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7afaecb1-96be-4da1-ae3c-1ce005fd9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y labels\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(words)-6):\n",
    "    first_five = words[i:i+5]\n",
    "    next = words[i+5:i+6]\n",
    "\n",
    "    x.append(first_five)\n",
    "    y.append(next[0])\n",
    "    #print(first_five)\n",
    "    #print(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a250fcc6-9b72-45ee-a457-24d8750d22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "train_x = x[:int(len(x)*0.8)]\n",
    "test_x = x[len(train_x)-1:]\n",
    "\n",
    "train_y = y[:int(len(y)*0.8)]\n",
    "test_y = y[len(train_y)-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b03b726b-7239-4860-9b56-50fc51f6fc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15185, 15185, 3798, 3798)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(train_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0fff0-c10b-411a-a5d3-bdff8c46c05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9219a26a-74f2-4a28-95d7-c39d5e7c6f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHidden units: 50\\nm: 60\\nn: 5\\ndirect: yes\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model config\n",
    "\"\"\"\n",
    "Hidden units: 50\n",
    "m: 60\n",
    "n: 5\n",
    "direct: yes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8129a1b4-f6d4-45d7-bf42-8db5db618a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer:  torch.Size([300, 50])\n",
      "U layer:  torch.Size([50, 18988])\n",
      "Direct representation layer:  torch.Size([300, 18988])\n",
      "C matrix:  torch.Size([18988, 60])\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "# multiple sequence of words as input\n",
    "feature_vector_len = 60\n",
    "hidden_units = 50\n",
    "vocab = len(words)\n",
    "n = 5\n",
    "\n",
    "hidden_layer = torch.randn(n*feature_vector_len, hidden_units, requires_grad=True)\n",
    "U = torch.randn(hidden_units, vocab, requires_grad=True)\n",
    "direct_layer = torch.randn(n*feature_vector_len, vocab, requires_grad=True)\n",
    "C = torch.randn(vocab, feature_vector_len, requires_grad=True)\n",
    "d = torch.tensor(1.0,requires_grad=True)\n",
    "b = torch.tensor(1.0,requires_grad=True)\n",
    "optimizer = optim.SGD([C, direct_layer, hidden_layer, U, d,b], lr=0.1, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "print(\"hidden layer: \", hidden_layer.shape)\n",
    "print(\"U layer: \", U.shape)\n",
    "print(\"Direct representation layer: \", direct_layer.shape)\n",
    "print(\"C matrix: \", C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c0516f-b106-4cbe-bb0a-4bf55f44e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence:  ['Godfrey', 'name', 'cashier', \"fears',\", 'good-night']\n",
      "next word:  waddling\n",
      "feature vectors: torch.Size([1, 300])\n",
      "label:  torch.Size([60])\n",
      "\n",
      "Input @ Hidden layer\n",
      "layer 1 output: torch.Size([1, 50])\n",
      "\n",
      " Output from layer 1 @ Output layer\n",
      "layer 2 output: torch.Size([1, 18988])\n",
      "\n",
      " Input @ Direct rep\n",
      "Direct rep output: torch.Size([1, 18988])\n",
      "\n",
      "Final output - layer 2 + direct: torch.Size([1, 18988])\n",
      "\n",
      "softmax output: torch.Size([1, 18988])\n",
      "\n",
      "prediction: 'villagers\n",
      "78.93074798583984\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax(dim=1)\n",
    "CLE = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_feature_vector(word):\n",
    "    index = word_to_i[word]\n",
    "    return C[index]\n",
    "    \n",
    "print(\"input sequence: \",train_x[0])\n",
    "print(\"next word: \",train_y[0])\n",
    "\n",
    "feature_vectors = torch.stack([get_feature_vector(word) for word in train_x[0]])\n",
    "feature_vectors = torch.cat(torch.unbind(feature_vectors), dim=0)\n",
    "feature_vectors = feature_vectors.view(1,-1)\n",
    "print(\"feature vectors:\", feature_vectors.shape)\n",
    "\n",
    "label = get_feature_vector(train_y[0])\n",
    "print(\"label: \", label.shape)\n",
    "\n",
    "# Hidden layer\n",
    "\n",
    "layer_1_output = torch.matmul(feature_vectors, hidden_layer) + d \n",
    "print(\"\\nInput @ Hidden layer\")\n",
    "print(\"layer 1 output:\", layer_1_output.shape)\n",
    "\n",
    "layer_1_output = tanh(layer_1_output)\n",
    "\n",
    "# Hidden to output layer\n",
    "layer_2_output = torch.matmul(layer_1_output, U)\n",
    "print(\"\\n Output from layer 1 @ Output layer\")\n",
    "print(\"layer 2 output:\", layer_2_output.shape)\n",
    "\n",
    "\n",
    "# Direct representation\n",
    "\n",
    "direct_output = torch.matmul(feature_vectors, direct_layer) + b\n",
    "print(\"\\n Input @ Direct rep\")\n",
    "print(\"Direct rep output:\", direct_output.shape)\n",
    "\n",
    "# Concat\n",
    "final_output = layer_2_output + direct_output\n",
    "print(\"\\nFinal output - layer 2 + direct:\", final_output.shape)\n",
    "\n",
    "# Softmax\n",
    "prob = softmax(final_output)\n",
    "print(\"\\nsoftmax output:\", prob.shape)\n",
    "\n",
    "answer = torch.argmax(prob)\n",
    "print(\"\\nprediction:\", i_to_word[answer.item()])\n",
    "\n",
    "# Loss\n",
    "loss = CLE(final_output, torch.tensor([word_to_i[train_y[0]]]))\n",
    "print(loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df6c2358-cb5d-4662-a412-4a2573334b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(word):\n",
    "    index = word_to_i[word]\n",
    "    return C[index]\n",
    "    \n",
    "def get_batch(x,y, size):\n",
    "    \n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    # Get list of 50 random indexes\n",
    "    res = random.sample(range(0, len(x)), size)\n",
    "    \n",
    "    for i in res:\n",
    "        # Get feature vectors for all 5 words, combined into one vector\n",
    "        feature_vectors = torch.stack([get_feature_vector(word) for word in x[i]])\n",
    "        feature_vectors = torch.cat(torch.unbind(feature_vectors), dim=0)\n",
    "        #feature_vectors = feature_vectors.view(1,-1) # [1,n*m]\n",
    "        batch_x.append(feature_vectors)\n",
    "        batch_y.append(word_to_i[y[i]])\n",
    "        \n",
    "    batch_x = torch.stack(batch_x)\n",
    "    batch_y = torch.tensor(batch_y)\n",
    "    # Y label doesn't require feature vectors, loss is calculated directly using index\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3e55550-185e-4009-ab09-ad216b290016",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = get_batch(train_x,train_y,len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd0b890b-58a8-42b8-aaa2-1baeb0d78053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15185, 300]), torch.Size([15185]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a619fcd1-b2d8-454a-a4d1-44b141558108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 74.11334991455078\n",
      "Loss: 73.98956298828125\n",
      "Loss: 73.86213684082031\n",
      "Loss: 73.73189544677734\n",
      "Loss: 73.5995864868164\n",
      "Loss: 73.4657974243164\n",
      "Loss: 73.33113098144531\n",
      "Loss: 73.19601440429688\n",
      "Loss: 73.06089782714844\n",
      "Loss: 72.92610168457031\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax(dim=1)\n",
    "CLE = nn.CrossEntropyLoss()\n",
    "                          \n",
    "for i in range(epoch):\n",
    "\n",
    "    batch_x,batch_y = get_batch(train_x,train_y, len(train_x))\n",
    "\n",
    "    # Hidden layer\n",
    "    layer_1_output = torch.matmul(batch_x, hidden_layer) + d\n",
    "    layer_1_output = tanh(layer_1_output)\n",
    "    \n",
    "    # Hidden to output layer\n",
    "    layer_2_output = torch.matmul(layer_1_output, U)\n",
    "\n",
    "    # Direct representation\n",
    "    direct_output = torch.matmul(batch_x, direct_layer) + b\n",
    "\n",
    "    # Concat\n",
    "    final_output = layer_2_output + direct_output\n",
    "    #print(final_output.shape)\n",
    "\n",
    "    # Loss\n",
    "    loss = CLE(final_output, batch_y)\n",
    "    print(\"Loss:\",loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24a518aa-d4e6-4c64-8d32-50bbce154cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: [\"afterwards?”',\", \"becomes',\", 'better.', \"round',\", 'now.']\n",
      "answer: thoughtfully',\n",
      "prediction: 'Windigate\n",
      "\n",
      "\n",
      "sequence: [\"becomes',\", 'better.', \"round',\", 'now.', \"thoughtfully',\"]\n",
      "answer: oak',\n",
      "prediction: one',\n",
      "\n",
      "\n",
      "sequence: ['better.', \"round',\", 'now.', \"thoughtfully',\", \"oak',\"]\n",
      "answer: bird,”\n",
      "prediction: '“Evidently,”\n",
      "\n",
      "\n",
      "sequence: [\"round',\", 'now.', \"thoughtfully',\", \"oak',\", 'bird,”']\n",
      "answer: 'good-night.’\n",
      "prediction: boot',\n",
      "\n",
      "\n",
      "sequence: ['now.', \"thoughtfully',\", \"oak',\", 'bird,”', \"'good-night.’\"]\n",
      "answer: 'accomplishment.\n",
      "prediction: “The\n",
      "\n",
      "\n",
      "sequence: [\"thoughtfully',\", \"oak',\", 'bird,”', \"'good-night.’\", \"'accomplishment.\"]\n",
      "answer: shock',\n",
      "prediction: shade.\n",
      "\n",
      "\n",
      "sequence: [\"oak',\", 'bird,”', \"'good-night.’\", \"'accomplishment.\", \"shock',\"]\n",
      "answer: clad\n",
      "prediction: S.\n",
      "\n",
      "\n",
      "sequence: ['bird,”', \"'good-night.’\", \"'accomplishment.\", \"shock',\", 'clad']\n",
      "answer: jealously,',\n",
      "prediction: hanging\n",
      "\n",
      "\n",
      "sequence: [\"'good-night.’\", \"'accomplishment.\", \"shock',\", 'clad', \"jealously,',\"]\n",
      "answer: room',\n",
      "prediction: '_Echo_',\n",
      "\n",
      "\n",
      "sequence: [\"'accomplishment.\", \"shock',\", 'clad', \"jealously,',\", \"room',\"]\n",
      "answer: 'forward',\n",
      "prediction: Service.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "def get_predictions(x,y):\n",
    "    batch_x,batch_y = get_batch(x,y,len(x))\n",
    "    \n",
    "    layer_1_output = torch.matmul(batch_x, hidden_layer) + d\n",
    "    layer_1_output = tanh(layer_1_output)\n",
    "    \n",
    "    # Hidden to output layer\n",
    "    layer_2_output = torch.matmul(layer_1_output, U)\n",
    "\n",
    "    # Direct representation\n",
    "    direct_output = torch.matmul(batch_x, direct_layer) + b\n",
    "\n",
    "    # Concat\n",
    "    final_output = layer_2_output + direct_output\n",
    "    #print(final_output.shape)\n",
    "    \n",
    "    # Softmax\n",
    "    prob = softmax(final_output)\n",
    "    for i in range(prob.shape[0]):\n",
    "        print(\"sequence:\", x[i])\n",
    "        print(\"answer:\", y[i])\n",
    "        answer = torch.argmax(prob[i])\n",
    "        print(\"prediction:\", i_to_word[answer.item()])\n",
    "        print(\"\\n\")\n",
    "\n",
    "get_predictions(test_x[:10], test_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb1f33c0-46b5-4827-9d70-bc30c67b3428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Determining learning rate\n",
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aece0b-95dc-4b90-ac4a-f04c51a7a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single iteration with all 1000 learning rates and log loss\n",
    "\n",
    "lri = []\n",
    "lossi = []\n",
    "for i in range(epoch):\n",
    "\n",
    "    batch_x,batch_y = get_batch(train_x,train_y, len(train_x))\n",
    "\n",
    "    # Hidden layer\n",
    "    layer_1_output = torch.matmul(batch_x, hidden_layer) + d\n",
    "    layer_1_output = tanh(layer_1_output)\n",
    "    \n",
    "    # Hidden to output layer\n",
    "    layer_2_output = torch.matmul(layer_1_output, U)\n",
    "\n",
    "    # Direct representation\n",
    "    direct_output = torch.matmul(batch_x, direct_layer) + b\n",
    "\n",
    "    # Concat\n",
    "    final_output = layer_2_output + direct_output\n",
    "    #print(final_output.shape)\n",
    "\n",
    "    # Loss\n",
    "    loss = CLE(final_output, batch_y)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
