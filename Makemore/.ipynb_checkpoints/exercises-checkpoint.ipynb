{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72676aad-4c6f-4155-9fac-e7239dfb42da",
   "metadata": {},
   "source": [
    "\n",
    "Exercises:\n",
    "- E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "- E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "- E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "- E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels - wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "- E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "- E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca9955-e891-4228-8b9d-c42a00867292",
   "metadata": {},
   "source": [
    "### Tri-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc26cc9-68d0-4109-9030-73b286196f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd import MLP\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ac3d9-fc76-46ae-85dd-44e587e1598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3a5c8-8f93-414f-bf99-4685b09cda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(words[:1])):\n",
    "    if len(words) >= 3:\n",
    "        temp = '.' + words[j] + '.'\n",
    "        for i in range(len(temp)-2):\n",
    "            print(\"prev:\", temp[i:i+2])\n",
    "            print(\"next:\", temp[i+2:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f54ca7-e865-48bf-b374-bf5c5a7aac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "char = sorted(list(set(''.join(words))))\n",
    "char = ['.'] + char\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a3380-3353-488f-90d3-389757f99c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "for i in range(len(char)):\n",
    "    stoi[char[i]] = i\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fc793-3aaa-45da-8129-a0d7eff3d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "itoi = {}\n",
    "for i in range(len(char)):\n",
    "    itoi[i] = char[i]\n",
    "itoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c3f1c-ac0f-4060-91cf-ee665b8c8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(words[:1])):\n",
    "    if len(words) >= 3:\n",
    "        words[j] = '.' + words[j] + '.'\n",
    "        for i in range(len(words[j])-2):\n",
    "            prev = list(words[j][i:i+2])\n",
    "            after = words[j][i+2:i+3]\n",
    "            print(\"prev:\", stoi[prev[0]], stoi[prev[1]])\n",
    "            print(\"next:\",stoi[after])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6ffe4-b63f-4087-ba0c-aa2a9ce13cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset dataset\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf70aa9-9e9c-42de-b6d7-bf0f2e3a44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for j in range(len(words)):\n",
    "    if len(words) >= 3:\n",
    "        words[j] = '.' + words[j] + '.'\n",
    "        for i in range(len(words[j])-2):\n",
    "            prev = list(words[j][i:i+2])\n",
    "            after = words[j][i+2:i+3]\n",
    "            prev_i = [stoi[prev[0]], stoi[prev[1]]]\n",
    "            after_i = stoi[after]\n",
    "            xs.append(prev_i)\n",
    "            ys.append(after_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fad192-f812-4f81-b594-6cf41f3fb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1c7f4-4ccc-4a6f-993b-7a0481fd498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cdfc9a-da7a-42b2-a406-7de997906745",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8363a9-36ea-4bdb-a8d9-68dfba22162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = F.one_hot(xs, num_classes=27).float()\n",
    "ys = F.one_hot(ys, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4f63c-df81-4aff-a049-0a5cd5d6c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46917fd2-6a99-4a2c-98eb-79153f01c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9db3b-0ffc-4b3e-9a96-923da3438764",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_combined = []\n",
    "for i in range(xs.shape[0]):\n",
    "    xs_combined.append(torch.cat((xs[i][0], xs[i][1]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccccbbab-f451-4621-a652-f88442ea7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_combined = torch.stack(xs_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb5760-23a1-43d4-91f4-e6291388d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs_combined.shape)\n",
    "print(xs_combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f079c4-9a1d-4aba-9c20-555b0931fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad8e17-49cc-4281-9654-70f314ac6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_combined.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63719c2f-2bcd-455a-bd4f-7635efeceb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP(27+27,[256,128,27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa3e9f-2cb0-4189-b120-e09852b98e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nn(xs_combined[:10], activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923d67f-8f62-4b77-84a9-0b18f919f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"output shape:\", len(output), len(output[0]), \"\\n\")\n",
    "print(len(output[0]), output[0])\n",
    "print(\"\\n\",sum(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de7bdd-576f-4131-a6e3-b3fdf307f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.cross_entropy_loss(output, ys[:10])\n",
    "print(\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057a637-ad17-438b-85c3-c817c4d08416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c653a96-fda0-4a38-9684-ca9f494e2965",
   "metadata": {},
   "source": [
    "##### E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907bbca-5ce7-4c18-a7d7-c92b62731bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset dataset\n",
    "\n",
    "print(\"dataset x:\", xs.shape)\n",
    "print(\"dataset y:\",ys.shape)\n",
    "\n",
    "train_percent = int((.8 * xs.shape[0]))\n",
    "train_set_xs = xs[:train_percent]\n",
    "\n",
    "rest_xs = xs[train_percent+1:]\n",
    "dev_set_xs, test_set_xs = rest_xs[:len(rest_xs)//2], rest_xs[len(rest_xs)//2+1:]\n",
    "\n",
    "print(\"\\ntrain_set_xs: \", train_set_xs.shape)\n",
    "print(\"dev_set_xs: \", dev_set_xs.shape)\n",
    "print(\"test_set_xs: \", test_set_xs.shape)\n",
    "\n",
    "train_set_ys = ys[:train_percent]\n",
    "\n",
    "rest_ys = ys[train_percent+1:]\n",
    "dev_set_ys, test_set_ys = rest_ys[:len(rest_ys)//2], rest_ys[len(rest_ys)//2+1:]\n",
    "\n",
    "print(\"\\ntrain_set_ys: \", train_set_ys.shape)\n",
    "print(\"dev_set_ys: \", dev_set_ys.shape)\n",
    "print(\"test_set_ys: \", test_set_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e69b87-881f-4c09-9f1f-11ac5eb6d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP(27+27,[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34243a5-6535-420e-9361-837a975dda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 10\n",
    "\n",
    "def l1_reg(x):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += x[i]**2\n",
    "\n",
    "    # mean\n",
    "    return loss/len(x)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "\n",
    "    logits = nn(xs_combined[:10], activation=\"softmax\")\n",
    "    reg_loss = sum([0.01 * l1_reg(logit) for logit in logits]) / len(logits)\n",
    "    print(\"reguralization loss:\", reg_loss.data)\n",
    "    loss = nn.cross_entropy_loss(logits, ys[:10])\n",
    "\n",
    "    loss += reg_loss\n",
    "    \n",
    "    print(\"loss: \", loss.data)\n",
    "    \n",
    "    # zero grad\n",
    "    for p in nn.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward() \n",
    "    for p in nn.parameters():\n",
    "        p.data += -lr * p.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e1c2b-6a1f-433b-8b1e-f2f565480784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_char_from_one_hot(one_hot):\n",
    "    for i in range(len(one_hot)):\n",
    "        if one_hot[i] == 1:\n",
    "            return itoi[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b839fa7-cee6-4704-81ae-c9e68e286ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    input = [torch.cat((test_set_xs[i][0], test_set_xs[i][1]), dim=0)]\n",
    "    logits = nn(input, activation=\"softmax\")\n",
    "    logits = logits[0] \n",
    "    max = logits[0].data\n",
    "    index = 0\n",
    "    for j in range(len(logits)):\n",
    "        if logits[j].data > max:\n",
    "            max = logits[j].data\n",
    "            index = j\n",
    "    \n",
    "    print(\"characters:\", get_char_from_one_hot(test_set_xs[i][0]), get_char_from_one_hot(test_set_xs[i][1]))\n",
    "    print(\"next char prediction:\", itoi[index])\n",
    "    print(\"actual answer:\", get_char_from_one_hot(test_set_ys[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3260d-4eca-40b9-a51d-e633197f5383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b3fd5-5455-4f9a-a03e-9538f000d6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3dd42-ea7e-4dad-9266-ad915c5cc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54 input -> 1 neuron -> 1 output\n",
    "W = torch.randn((54,1))\n",
    "\n",
    "train_set_combined = torch.cat((train_set_xs[0][0],train_set_xs[0][1]), axis=0)\n",
    "\n",
    "print(\"input shape:\",train_set_combined.shape)\n",
    "print(train_set_combined)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"layer shape:\", W.shape)\n",
    "#print(\"layer:\", W)\n",
    "print(\"\\n\")\n",
    "forward = torch.matmul(train_set_combined, W)\n",
    "print(\"output shape -> input * W:\",forward.shape)\n",
    "print(forward)\n",
    "# Single neuron of 54 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51994c17-2865-4b8b-993a-513bf6c2f97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe9f0e-df65-4623-915b-92a22c7c84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54 input -> 27 NEURONS -> 27 output vector\n",
    "W = torch.randn((54,27))\n",
    "\n",
    "train_set_combined = [torch.cat((x[0],x[1]), axis=0) for x in train_set_xs[:1]]\n",
    "train_set_combined = torch.stack(train_set_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36017c65-0930-4f0c-ae5e-9b15f5b9fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input shape:\",train_set_combined.shape)\n",
    "print(train_set_combined)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"layer shape:\", W.shape)\n",
    "print(\"layer:\", W)\n",
    "print(\"\\n\")\n",
    "forward = torch.matmul(train_set_combined, W)\n",
    "print(\"output shape -> input * W:\",forward.shape)\n",
    "print(forward)\n",
    "# Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f2d10-1041-4b58-bfed-7d9a036d3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_combined = [torch.cat((x[0],x[1]), axis=0) for x in train_set_xs[:100]]\n",
    "train_set_combined = torch.stack(train_set_combined)\n",
    "print(\"input shape:\",train_set_combined.shape)\n",
    "print(train_set_combined)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"layer shape:\", W.shape)\n",
    "print(\"layer:\", W)\n",
    "print(\"\\n\")\n",
    "forward = torch.matmul(train_set_combined, W)\n",
    "print(\"output shape -> input * W:\",forward.shape)\n",
    "print(forward)\n",
    "# Doesn't matter if there are multiple inputs, output will be (no of inputs, neuron output size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35116f00-2158-4274-85d4-d541b6476e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    counts = [logit.exp() for logit in x]\n",
    "    denominator = sum(counts)\n",
    "    out = [c / denominator for c in counts]\n",
    "    \n",
    "    return out\n",
    "\n",
    "softmax_layer = []\n",
    "for i in range(len(forward)):\n",
    "    softmax_layer.append(softmax(forward[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63173409-4271-45e8-8eb2-520cc10807d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor(softmax_layer[0]))\n",
    "print(sum(softmax_layer[0]))\n",
    "print(len(softmax_layer[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd138183-00e3-4b4f-8797-bf81de03000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 of 54 vector input -> 54 of 128 neurons -> 100 of 128 output vector\n",
    "# 100 of 128 vector input -> 128 of 64 neurons -> 100 of 64 output vector \n",
    "# 100 of 64 vector input -> 64 of 27 neurons -> 100 of 27 output vector \n",
    "\n",
    "layer_1 = torch.randn((54,128)) # Layer(54,128)\n",
    "layer_2 = torch.randn((128,64)) # Layer(128, 64)\n",
    "layer_3 = torch.randn((64,27)) # Layer(64,27)\n",
    "\n",
    "layer_1_output = torch.matmul(train_set_combined,layer_1)\n",
    "layer_2_output = torch.matmul(layer_1_output,layer_2)\n",
    "layer_3_output = torch.matmul(layer_2_output,layer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a10633-750b-4cf9-95dd-b771ba66c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_1_output.shape)\n",
    "print(layer_2_output.shape)\n",
    "print(layer_3_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da6342-8f8e-45e4-a21b-286a64e3f6c7",
   "metadata": {},
   "source": [
    "##### E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels - wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7091dd-944b-4f1d-a688-8b8af5e46e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it just selects the row of weight matrix, basically if one hot vector is 1 at 4th index row wise, it is practically returning\n",
    "# the 4th row of weight matrix(layer matrix), since all the 0s multiplying with other items don't count, we end just multiplying\n",
    "# 1 * 4th number of each neuron(column)\n",
    "\n",
    "one_hot_27 = train_set_combined[0][:27]\n",
    "layer_27_neuron = torch.randn((27,27))\n",
    "\n",
    "print(one_hot_27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7800d2e-349e-4615-b27f-9f1d34533e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = torch.matmul(one_hot_27, layer_27_neuron)\n",
    "layer_row_0th = layer_27_neuron[0] # 0th index at one hot is 1\n",
    "\n",
    "print(forward)\n",
    "print(layer_row_0th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c2d5c-f033-43e9-861c-377e15db3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward == layer_row_0th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291b2dc-10c5-4856-91ab-6950d2b1756c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193410eb-085d-4fde-97e7-d89d6a6ab4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42f5e416-03e5-4e03-8d2e-bd1f48201902",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232e611-14c9-42b0-9520-41b2502f00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_27 = torch.randn((54,27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31383890-56ae-4c9d-a05c-4dd34914dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = xs_combined[:50]\n",
    "sample_label = test_set_ys[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71525c98-2657-49d6-b576-507ac7586920",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.matmul(sample_train, layer_27)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc4603-fc7b-4124-9d62-ad7dbe2fb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    counts = [logit.exp() for logit in x]\n",
    "    denominator = sum(counts)\n",
    "    out = [c / denominator for c in counts]\n",
    "    \n",
    "    return torch.tensor(out)\n",
    "    \n",
    "prob = [softmax(logit) for logit in logits]\n",
    "prob = torch.stack(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c04bc-6fe7-46e4-ace9-eb806bd7a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob.shape)\n",
    "print(prob[0])\n",
    "print(sum(prob[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423d219-25f3-413f-8891-eb6e6d8c95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(x,y):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += y[i] * x[i].log()\n",
    "    return (-loss)\n",
    "    \n",
    "loss = sum(cross_entropy_loss(x,y) for x,y in zip(prob, sample_label)) / 50\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc877b-950a-46c8-b3a5-13541f04af62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edbf44-344b-4667-99cc-4ecb2e7a8840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742b3e6-da97-4423-a6d1-68a58304a2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bad3b2ea-dbee-46ce-9ac9-c352281ebe4a",
   "metadata": {},
   "source": [
    "##### E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c0f8c-41d1-4225-bb65-0c71e09f29cf",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cad99-9c9a-4bb3-8ba5-bbae5495ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 10\n",
    "train_set_combined = [torch.cat((x[0],x[1]), axis=0) for x in train_set_xs[:100]]\n",
    "\n",
    "def l1_reg(x):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += x[i]**2\n",
    "\n",
    "    # mean\n",
    "    return loss/len(x)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "\n",
    "    logits = nn(train_set_combined[:10], activation=\"softmax\")\n",
    "    reg_loss = sum([0.2 * l1_reg(logit) for logit in logits]) / len(logits)\n",
    "    print(\"reguralization loss:\", reg_loss.data)\n",
    "    loss = nn.cross_entropy_loss(logits, train_set_ys[:10])\n",
    "\n",
    "    loss += reg_loss\n",
    "    \n",
    "    print(\"loss: \", loss.data)\n",
    "    \n",
    "    # zero grad\n",
    "    for p in nn.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward() \n",
    "    for p in nn.parameters():\n",
    "        p.data += -lr * p.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49c016-b65d-4fed-9825-e59244932d76",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fff32-13c8-43c5-b7fc-69c08a11516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_combined = [torch.cat((x[0],x[1]), axis=0) for x in test_set_xs[:100]]\n",
    "    \n",
    "logits = nn(test_set_combined[:10], activation=\"softmax\")\n",
    "loss = nn.cross_entropy_loss(logits, test_set_ys[:10])\n",
    "print(\"loss: \", loss.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1076740-41d0-4944-b0f6-f11ec06c86c8",
   "metadata": {},
   "source": [
    "##### Decreasing importance of regularization by reduce paramater results in increased training accuracy but no difference in testing accuracy\n",
    "##### Increaseing regularization parameters resulting in about the same training accuracy but higher accuracy on testing, showcasing more generalization and better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d5180-bd3b-46c8-be99-ea384aa6cd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
