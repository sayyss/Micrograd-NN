{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72676aad-4c6f-4155-9fac-e7239dfb42da",
   "metadata": {},
   "source": [
    "\n",
    "Exercises:\n",
    "- E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "- E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "- E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "- E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels - wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "- E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "- E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca9955-e891-4228-8b9d-c42a00867292",
   "metadata": {},
   "source": [
    "### Tri-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fc26cc9-68d0-4109-9030-73b286196f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd import MLP\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "295ac3d9-fc76-46ae-85dd-44e587e1598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cb3a5c8-8f93-414f-bf99-4685b09cda47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev: .e\n",
      "next: m\n",
      "prev: em\n",
      "next: m\n",
      "prev: mm\n",
      "next: a\n",
      "prev: ma\n",
      "next: .\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(words[:1])):\n",
    "    if len(words) >= 3:\n",
    "        temp = '.' + words[j] + '.'\n",
    "        for i in range(len(temp)-2):\n",
    "            print(\"prev:\", temp[i:i+2])\n",
    "            print(\"next:\", temp[i+2:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39f54ca7-e865-48bf-b374-bf5c5a7aac66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = sorted(list(set(''.join(words))))\n",
    "char = ['.'] + char\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a9a3380-3353-488f-90d3-389757f99c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {}\n",
    "for i in range(len(char)):\n",
    "    stoi[char[i]] = i\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "935fc793-3aaa-45da-8129-a0d7eff3d2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '.',\n",
       " 1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itoi = {}\n",
    "for i in range(len(char)):\n",
    "    itoi[i] = char[i]\n",
    "itoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c52c3f1c-ac0f-4060-91cf-ee665b8c8a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev: 0 5\n",
      "next: 13\n",
      "prev: 5 13\n",
      "next: 13\n",
      "prev: 13 13\n",
      "next: 1\n",
      "prev: 13 1\n",
      "next: 0\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(words[:1])):\n",
    "    if len(words) >= 3:\n",
    "        words[j] = '.' + words[j] + '.'\n",
    "        for i in range(len(words[j])-2):\n",
    "            prev = list(words[j][i:i+2])\n",
    "            after = words[j][i+2:i+3]\n",
    "            print(\"prev:\", stoi[prev[0]], stoi[prev[1]])\n",
    "            print(\"next:\",stoi[after])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50b6ffe4-b63f-4087-ba0c-aa2a9ce13cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset dataset\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccf70aa9-9e9c-42de-b6d7-bf0f2e3a44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for j in range(len(words)):\n",
    "    if len(words) >= 3:\n",
    "        words[j] = '.' + words[j] + '.'\n",
    "        for i in range(len(words[j])-2):\n",
    "            prev = list(words[j][i:i+2])\n",
    "            after = words[j][i+2:i+3]\n",
    "            prev_i = [stoi[prev[0]], stoi[prev[1]]]\n",
    "            after_i = stoi[after]\n",
    "            xs.append(prev_i)\n",
    "            ys.append(after_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3fad192-f812-4f81-b594-6cf41f3fb8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cba1c7f4-4ccc-4a6f-993b-7a0481fd498a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63cdfc9a-da7a-42b2-a406-7de997906745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([196113, 2]), torch.Size([196113]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d8363a9-36ea-4bdb-a8d9-68dfba22162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = F.one_hot(xs, num_classes=27).float()\n",
    "ys = F.one_hot(ys, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bc4f63c-df81-4aff-a049-0a5cd5d6c701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46917fd2-6a99-4a2c-98eb-79153f01c0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92d9db3b-0ffc-4b3e-9a96-923da3438764",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_combined = []\n",
    "for i in range(xs.shape[0]):\n",
    "    xs_combined.append(torch.cat((xs[i][0], xs[i][1]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccccbbab-f451-4621-a652-f88442ea7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_combined = torch.stack(xs_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ecb5760-23a1-43d4-91f4-e6291388d6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196113, 54])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(xs_combined.shape)\n",
    "print(xs_combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4f079c4-9a1d-4aba-9c20-555b0931fe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20ad8e17-49cc-4281-9654-70f314ac6544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([196113, 54]), torch.Size([196113, 27]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_combined.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63719c2f-2bcd-455a-bd4f-7635efeceb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP(27+27,[256,128,27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcfa3e9f-2cb0-4189-b120-e09852b98e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "output = nn(xs_combined[:10], activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8923d67f-8f62-4b77-84a9-0b18f919f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: 10 27 \n",
      "\n",
      "27 [Value(data=0.06676130969401942, grad=0.0), Value(data=0.009079568342766015, grad=0.0), Value(data=0.06708854209468193, grad=0.0), Value(data=0.06669485239783271, grad=0.0), Value(data=0.06708942817219318, grad=0.0), Value(data=0.06707802573033872, grad=0.0), Value(data=0.06708943983705477, grad=0.0), Value(data=0.015617456442614383, grad=0.0), Value(data=0.009239423088165217, grad=0.0), Value(data=0.009139958654519695, grad=0.0), Value(data=0.06708943247718163, grad=0.0), Value(data=0.009309121956545294, grad=0.0), Value(data=0.009079568342797363, grad=0.0), Value(data=0.009079643882747911, grad=0.0), Value(data=0.009079828905049601, grad=0.0), Value(data=0.0670894398377483, grad=0.0), Value(data=0.0664424537373773, grad=0.0), Value(data=0.009175716785548385, grad=0.0), Value(data=0.06705835889698941, grad=0.0), Value(data=0.009079568342663772, grad=0.0), Value(data=0.009089081770884494, grad=0.0), Value(data=0.06708903688552054, grad=0.0), Value(data=0.01900114988228787, grad=0.0), Value(data=0.009079961519658526, grad=0.0), Value(data=0.05221085683799904, grad=0.0), Value(data=0.009079571697660124, grad=0.0), Value(data=0.0670892037871543, grad=0.0)]\n",
      "\n",
      " Value(data=1.0, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"output shape:\", len(output), len(output[0]), \"\\n\")\n",
    "print(len(output[0]), output[0])\n",
    "print(\"\\n\",sum(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9de7bdd-576f-4131-a6e3-b3fdf307f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=4.701720237731934, grad=0.0), Value(data=4.531747817993164, grad=0.0), Value(data=3.7306344509124756, grad=0.0), Value(data=3.667306423187256, grad=0.0), Value(data=4.799770832061768, grad=0.0), Value(data=4.819977283477783, grad=0.0), Value(data=2.789294958114624, grad=0.0), Value(data=4.67116641998291, grad=0.0), Value(data=3.3350625038146973, grad=0.0), Value(data=2.6640658378601074, grad=0.0)]\n",
      "number of loss: 10\n"
     ]
    }
   ],
   "source": [
    "loss = nn.cross_entropy_loss(output, ys[:10])\n",
    "print(loss)\n",
    "print(\"number of loss:\",len(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e3155f1-b730-467b-bf7c-46b12a3d3954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'Value' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m label \u001b[38;5;241m=\u001b[39m ys[i]\n\u001b[1;32m      6\u001b[0m logits \u001b[38;5;241m=\u001b[39m nn([\u001b[38;5;28minput\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(logits[\u001b[38;5;241m0\u001b[39m], label)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# zero grad\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/learning/Makemore/micrograd.py:201\u001b[0m, in \u001b[0;36mMLP.cross_entropy_loss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)):\n\u001b[1;32m    200\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x[j])):\n\u001b[1;32m    202\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y[j][i] \u001b[38;5;241m*\u001b[39m x[j][i]\u001b[38;5;241m.\u001b[39mlog()\n\u001b[1;32m    203\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39mloss)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Value' has no len()"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "for i in range(len(xs[:10])):\n",
    "    input = torch.cat((xs[i][0], xs[i][1]), dim=0)\n",
    "    label = ys[i]\n",
    "\n",
    "    logits = nn([input], activation=\"softmax\")\n",
    "    loss = nn.cross_entropy_loss(logits[0], label)\n",
    "    print(\"loss: \", loss)\n",
    "    \n",
    "    # zero grad\n",
    "    for p in nn.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    # backward   \n",
    "    loss.backward()\n",
    "    \n",
    "    for p in nn.parameters():\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4beed7-8c6e-40b1-85a7-bf808386c806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057a637-ad17-438b-85c3-c817c4d08416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c653a96-fda0-4a38-9684-ca9f494e2965",
   "metadata": {},
   "source": [
    "##### E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9907bbca-5ce7-4c18-a7d7-c92b62731bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset x: torch.Size([196113, 2, 27])\n",
      "dataset y: torch.Size([196113, 27])\n",
      "\n",
      "train_set_xs:  torch.Size([156890, 2, 27])\n",
      "dev_set_xs:  torch.Size([19611, 2, 27])\n",
      "test_set_xs:  torch.Size([19610, 2, 27])\n",
      "\n",
      "train_set_ys:  torch.Size([156890, 27])\n",
      "dev_set_ys:  torch.Size([19611, 27])\n",
      "test_set_ys:  torch.Size([19610, 27])\n"
     ]
    }
   ],
   "source": [
    "# reset dataset\n",
    "\n",
    "print(\"dataset x:\", xs.shape)\n",
    "print(\"dataset y:\",ys.shape)\n",
    "\n",
    "train_percent = int((.8 * xs.shape[0]))\n",
    "train_set_xs = xs[:train_percent]\n",
    "\n",
    "rest_xs = xs[train_percent+1:]\n",
    "dev_set_xs, test_set_xs = rest_xs[:len(rest_xs)//2], rest_xs[len(rest_xs)//2+1:]\n",
    "\n",
    "print(\"\\ntrain_set_xs: \", train_set_xs.shape)\n",
    "print(\"dev_set_xs: \", dev_set_xs.shape)\n",
    "print(\"test_set_xs: \", test_set_xs.shape)\n",
    "\n",
    "train_set_ys = ys[:train_percent]\n",
    "\n",
    "rest_ys = ys[train_percent+1:]\n",
    "dev_set_ys, test_set_ys = rest_ys[:len(rest_ys)//2], rest_ys[len(rest_ys)//2+1:]\n",
    "\n",
    "print(\"\\ntrain_set_ys: \", train_set_ys.shape)\n",
    "print(\"dev_set_ys: \", dev_set_ys.shape)\n",
    "print(\"test_set_ys: \", test_set_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34243a5-6535-420e-9361-837a975dda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "\n",
    "    inputs = [torch.cat((xs[i][0], xs[i][1]), dim=0) for xs in train_set_xs[:10]]\n",
    "    labels = train_set_ys[:10]\n",
    "\n",
    "    logits = nn(inputs, activation=\"softmax\")\n",
    "    local_loss = nn.cross_entropy_loss(logits, labels)\n",
    "    \n",
    "    \n",
    "    # zero grad\n",
    "    for p in nn.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    # backward   \n",
    "    local_loss.backward()\n",
    "    loss += local_loss\n",
    "    \n",
    "    for p in nn.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "print(\"loss: \", loss/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "373e1c2b-6a1f-433b-8b1e-f2f565480784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_char_from_one_hot(one_hot):\n",
    "    for i in range(len(one_hot)):\n",
    "        if one_hot[i] == 1:\n",
    "            return itoi[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b839fa7-cee6-4704-81ae-c9e68e286ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(test_set_xs[:10])):\n",
    "    \n",
    "    input = torch.cat((test_set_xs[i][0], test_set_xs[i][1]), dim=0)\n",
    "    logits = nn(input, activation=\"softmax\")\n",
    "    max = logits[0].data\n",
    "    index = 0\n",
    "    for j in range(len(logits)):\n",
    "        if logits[j].data > max:\n",
    "            max = logits[j].data\n",
    "            index = j\n",
    "    \n",
    "    print(\"characters:\", get_char_from_one_hot(test_set_xs[i][0]), get_char_from_one_hot(test_set_xs[i][1]))\n",
    "    print(\"next char prediction:\", itoi[index])\n",
    "    print(\"actual answer:\", get_char_from_one_hot(test_set_ys[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3260d-4eca-40b9-a51d-e633197f5383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b3fd5-5455-4f9a-a03e-9538f000d6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1d3dd42-ea7e-4dad-9266-ad915c5cc088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([54])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "layer shape: torch.Size([54, 1])\n",
      "\n",
      "\n",
      "output shape -> input * W: torch.Size([1])\n",
      "tensor([-0.2924])\n"
     ]
    }
   ],
   "source": [
    "# 54 input -> 1 neuron -> 1 output\n",
    "W = torch.randn((54,1))\n",
    "\n",
    "train_set_combined = torch.cat((train_set_xs[0][0],train_set_xs[0][1]), axis=0)\n",
    "\n",
    "print(\"input shape:\",train_set_combined.shape)\n",
    "print(train_set_combined)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"layer shape:\", W.shape)\n",
    "#print(\"layer:\", W)\n",
    "print(\"\\n\")\n",
    "forward = torch.matmul(train_set_combined, W)\n",
    "print(\"output shape -> input * W:\",forward.shape)\n",
    "print(forward)\n",
    "# Single neuron of 54 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51994c17-2865-4b8b-993a-513bf6c2f97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1fe9f0e-df65-4623-915b-92a22c7c84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54 input -> 27 NEURONS -> 27 output vector\n",
    "W = torch.randn((54,27))\n",
    "\n",
    "train_set_combined = [torch.cat((x[0],x[1]), axis=0) for x in train_set_xs[:1]]\n",
    "train_set_combined = torch.stack(train_set_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36017c65-0930-4f0c-ae5e-9b15f5b9fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 54])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "\n",
      "layer shape: torch.Size([54, 27])\n",
      "layer: tensor([[ 1.0635, -1.0277,  0.2941,  ..., -1.5864, -0.6445, -0.1288],\n",
      "        [-0.9908,  0.4976, -1.1646,  ..., -1.8226,  1.3122, -0.9762],\n",
      "        [ 0.6450, -0.6897, -0.8464,  ..., -1.3007, -0.6714, -1.4587],\n",
      "        ...,\n",
      "        [-0.7110,  0.2368, -1.1992,  ...,  0.5836, -0.0486,  1.0179],\n",
      "        [-0.3976,  1.1758, -1.8394,  ...,  1.1138, -0.6366, -1.6548],\n",
      "        [-0.7019, -0.2638,  0.1192,  ...,  0.8710,  0.5048,  0.0938]])\n",
      "\n",
      "\n",
      "output shape -> input * W: torch.Size([1, 27])\n",
      "tensor([[ 0.1861, -1.0648, -0.9486, -0.5944,  0.4486,  1.8571, -1.9449, -0.4053,\n",
      "          2.9364, -0.9600,  0.2285, -2.4027,  1.1911, -0.1877, -1.2792,  1.5652,\n",
      "         -1.3624, -2.4760, -1.3422,  0.7236, -1.1478, -2.1680, -1.8555, -3.7016,\n",
      "          0.0481,  1.3135,  0.4489]])\n"
     ]
    }
   ],
   "source": [
    "print(\"input shape:\",train_set_combined.shape)\n",
    "print(train_set_combined)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"layer shape:\", W.shape)\n",
    "print(\"layer:\", W)\n",
    "print(\"\\n\")\n",
    "forward = torch.matmul(train_set_combined, W)\n",
    "print(\"output shape -> input * W:\",forward.shape)\n",
    "print(forward)\n",
    "# Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e89f2d10-1041-4b58-bfed-7d9a036d3d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([100, 54])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      "\n",
      "layer shape: torch.Size([54, 27])\n",
      "layer: tensor([[ 1.0635, -1.0277,  0.2941,  ..., -1.5864, -0.6445, -0.1288],\n",
      "        [-0.9908,  0.4976, -1.1646,  ..., -1.8226,  1.3122, -0.9762],\n",
      "        [ 0.6450, -0.6897, -0.8464,  ..., -1.3007, -0.6714, -1.4587],\n",
      "        ...,\n",
      "        [-0.7110,  0.2368, -1.1992,  ...,  0.5836, -0.0486,  1.0179],\n",
      "        [-0.3976,  1.1758, -1.8394,  ...,  1.1138, -0.6366, -1.6548],\n",
      "        [-0.7019, -0.2638,  0.1192,  ...,  0.8710,  0.5048,  0.0938]])\n",
      "\n",
      "\n",
      "output shape -> input * W: torch.Size([100, 27])\n",
      "tensor([[ 0.1861, -1.0648, -0.9486,  ...,  0.0481,  1.3135,  0.4489],\n",
      "        [ 0.2379,  2.0707,  0.2634,  ...,  3.5340,  4.6433, -0.8014],\n",
      "        [-0.1425, -0.3339, -0.5058,  ...,  2.1013,  2.0914,  1.1610],\n",
      "        ...,\n",
      "        [-3.4606,  0.0328,  0.0074,  ...,  0.7086, -1.4903, -0.3706],\n",
      "        [-0.6128,  1.6456, -2.1013,  ...,  0.1413,  4.0424, -0.6816],\n",
      "        [-0.5153, -2.5138,  0.5286,  ...,  0.0126, -2.3873,  0.3585]])\n"
     ]
    }
   ],
   "source": [
    "train_set_combined = [torch.cat((x[0],x[1]), axis=0) for x in train_set_xs[:100]]\n",
    "train_set_combined = torch.stack(train_set_combined)\n",
    "print(\"input shape:\",train_set_combined.shape)\n",
    "print(train_set_combined)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"layer shape:\", W.shape)\n",
    "print(\"layer:\", W)\n",
    "print(\"\\n\")\n",
    "forward = torch.matmul(train_set_combined, W)\n",
    "print(\"output shape -> input * W:\",forward.shape)\n",
    "print(forward)\n",
    "# Doesn't matter if there are multiple inputs, output will be (no of inputs, neuron output size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35116f00-2158-4274-85d4-d541b6476e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    counts = [logit.exp() for logit in x]\n",
    "    denominator = sum(counts)\n",
    "    out = [c / denominator for c in counts]\n",
    "    \n",
    "    return out\n",
    "\n",
    "softmax_layer = []\n",
    "for i in range(len(forward)):\n",
    "    softmax_layer.append(softmax(forward[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63173409-4271-45e8-8eb2-520cc10807d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0238, 0.0068, 0.0076, 0.0109, 0.0309, 0.1265, 0.0028, 0.0132, 0.3722,\n",
      "        0.0076, 0.0248, 0.0018, 0.0650, 0.0164, 0.0055, 0.0945, 0.0051, 0.0017,\n",
      "        0.0052, 0.0407, 0.0063, 0.0023, 0.0031, 0.0005, 0.0207, 0.0734, 0.0309])\n",
      "tensor(1.0000)\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(softmax_layer[0]))\n",
    "print(sum(softmax_layer[0]))\n",
    "print(len(softmax_layer[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd138183-00e3-4b4f-8797-bf81de03000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 of 54 vector input -> 54 of 128 neurons -> 100 of 128 output vector\n",
    "# 100 of 128 vector input -> 128 of 64 neurons -> 100 of 64 output vector \n",
    "# 100 of 64 vector input -> 64 of 27 neurons -> 100 of 27 output vector \n",
    "\n",
    "layer_1 = torch.randn((54,128)) # Layer(54,128)\n",
    "layer_2 = torch.randn((128,64)) # Layer(128, 64)\n",
    "layer_3 = torch.randn((64,27)) # Layer(64,27)\n",
    "\n",
    "layer_1_output = torch.matmul(train_set_combined,layer_1)\n",
    "layer_2_output = torch.matmul(layer_1_output,layer_2)\n",
    "layer_3_output = torch.matmul(layer_2_output,layer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69a10633-750b-4cf9-95dd-b771ba66c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128])\n",
      "torch.Size([100, 64])\n",
      "torch.Size([100, 27])\n"
     ]
    }
   ],
   "source": [
    "print(layer_1_output.shape)\n",
    "print(layer_2_output.shape)\n",
    "print(layer_3_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da6342-8f8e-45e4-a21b-286a64e3f6c7",
   "metadata": {},
   "source": [
    "##### E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels - wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5d7091dd-944b-4f1d-a688-8b8af5e46e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Since it just selects the row of weight matrix, basically if one hot vector is 1 at 4th index row wise, it is practically returning\n",
    "# the 4th row of weight matrix(layer matrix), since all the 0s multiplying with other items don't count, we end just multiplying\n",
    "# 1 * 4th number of each neuron(column)\n",
    "\n",
    "one_hot_27 = train_set_combined[0][:27]\n",
    "layer_27_neuron = torch.randn((27,27))\n",
    "\n",
    "print(one_hot_27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d7800d2e-349e-4615-b27f-9f1d34533e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4642, -0.5628,  1.4523,  0.7894, -1.0376,  0.8167,  2.0185, -0.5827,\n",
      "        -0.3450, -0.1342, -0.3770, -0.4076,  0.5959, -0.9479,  0.4270, -0.4514,\n",
      "         1.5176, -1.2231, -1.3207, -0.7192,  0.9132, -0.1520, -0.3985,  1.1625,\n",
      "         0.9509,  1.0119,  0.8860])\n",
      "tensor([-1.4642, -0.5628,  1.4523,  0.7894, -1.0376,  0.8167,  2.0185, -0.5827,\n",
      "        -0.3450, -0.1342, -0.3770, -0.4076,  0.5959, -0.9479,  0.4270, -0.4514,\n",
      "         1.5176, -1.2231, -1.3207, -0.7192,  0.9132, -0.1520, -0.3985,  1.1625,\n",
      "         0.9509,  1.0119,  0.8860])\n"
     ]
    }
   ],
   "source": [
    "forward = torch.matmul(one_hot_27, layer_27_neuron)\n",
    "layer_row_0th = layer_27_neuron[0] # 0th index at one hot is 1\n",
    "\n",
    "print(forward)\n",
    "print(layer_row_0th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b2c2d5c-f033-43e9-861c-377e15db3f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward == layer_row_0th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291b2dc-10c5-4856-91ab-6950d2b1756c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193410eb-085d-4fde-97e7-d89d6a6ab4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42f5e416-03e5-4e03-8d2e-bd1f48201902",
   "metadata": {},
   "source": [
    "##### E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35b679-18f5-421e-b3be-3f4c6a601be3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31383890-56ae-4c9d-a05c-4dd34914dbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
