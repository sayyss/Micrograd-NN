{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a900d01-17da-43cb-9c9f-10fcca866074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import faiss\n",
    "from typing import List\n",
    "from langchain_core.runnables.config import run_in_executor\n",
    "import os\n",
    "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d126e8b8-8f84-4959-b302-a67ed6a7c2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "404438ec-3d2c-42cb-a62d-610ccf34d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-T5-738M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"MBZUAI/LaMini-T5-738M\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9f1789-1333-472e-89f6-ea58ae82be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = 'based on the given information: \"Parameter-Efficient Tuning Large Language Models for Graph\\nRepresentation Learning\\nQi Zhu\\nAmazon Web Services\\nqzhuamzn@amazon.com\\nDa Zheng\\nAmazon Web Services\\ndzzhen@amazon.com\\nXiang Song\\nAmazon Web Services\\ndzzhen@amazon.com\\nShichang Zhang‚àó\\nUniversity of California, Los Angeles\\nshichang@cs.ucla.edu\\nBowen Jin‚àó\\nUniversity of Illinois\\nUrbana-Champaign\\nbowenj4@illinois.edu\\nYizhou Sun\\nUniversity of California, Los Angeles\\nyzsun@cs.ucla.edu\\nGeorge Karypis\\nAmazon Web Services\\ngkarypis@amazon.com\\nABSTRACT\\nText-rich graphs, which exhibit rich textual information on nodes\\nand edges, are prevalent across a wide range of real-world business\\napplications. Large Language Models (LLMs) have demonstrated re-\\nmarkable abilities in understanding text, which also introduced the\\npotential for more expressive modeling in text-rich graphs. Despite\\nthese capabilities, efficiently applying LLMs to representation learn-\\ning on graphs presents significant challenges. Recently, parameter-\\nefficient fine-tuning methods for LLMs have enabled efficient new\\ntask generalization with minimal time and memory consumption.\\nInspired by this, we introduce Graph-aware Parameter-Efficient\\nFine-Tuning - GPEFT, a novel approach for efficient graph repre-\\nsentation learning with LLMs on text-rich graphs. Specifically, we\\nutilize a graph neural network (GNN) to encode structural infor-\\nmation from neighboring nodes into a graph prompt. This prompt\\nis then inserted at the beginning of the text sequence. To improve\\nthe quality of graph prompts, we pre-trained the GNN to assist the\\nfrozen LLM in predicting the next token in the node text. Compared\\nwith existing joint GNN and LMs, our method directly generates the\\nnode embeddings from large language models with an affordable\\nfine-tuning cost. We validate our approach through comprehen-\\nsive experiments conducted on 8 different text-rich graphs, observ-\\ning an average improvement of 2% in hit@1 and Mean Reciprocal\\nRank (MRR) in link prediction evaluations. Our results demonstrate\\nthe efficacy and efficiency of our model, showing that it can be\\nsmoothly integrated with various large language models, including\\nOPT, LLaMA, and Falcon.\\n‚àóWork done while being an intern at Amazon.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nConference‚Äô17, July 2017, Washington, DC, USA\\n¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\nKEYWORDS\\nRepresentation Learning, Graphs, Language Models\\nACM Reference Format:\\nQi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun,\\nand George Karypis. 2024. Parameter-Efficient Tuning Large Language\\nModels for Graph Representation Learning. In ACM, New York, NY, USA,\\n11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nInformation networks form the backbone of modern data systems:\\nmillions of daily social posts are shared among users on platforms\\nlike Facebook and Twitter; countless papers are published and cited\\nwithin academic networks. Many of these networks are rich in\\ntextual information on various types of objects, known as text-rich\\nor text-attributed networks. For example, in an e-commerce graph,\\ntext-rich information could be used to predict links by analyzing\\nproduct descriptions, user reviews, and metadata to recommend\\nitems that are frequently bought together. Such real-world applica-\\ntions typify the problem of link prediction, where representation\\nlearning (i.e., embedding) emerges as the most prevalent solution.\\nRepresentation learning on graph-structured data aims to learn\\na dense vector for each node through self-supervised tasks such as\\nlink prediction [9,34,46] or masked attribute recovery [14]. These\\ndense vectors can be widely utilized in various downstream ap-\\nplications including search, ranking, and retrieval. To effectively\\nutilize both node attributes and structural information in represen-\\ntation learning, Graph Neural Networks (GNNs) [11,21] devise a\\nnovel type of neural network with a message-passing mechanism.\\nIn text-rich graphs, raw text is usually transformed into feature\\nvectors by a text encoder prior to the application of Graph Neu-\\nral Networks (GNNs). Early text encoders, such as bag-of-words\\nand Word2Vec [32], are being gradually phased out due to signifi-\\ncant improvements in text representation from transformer-based\\nlanguage models (LMs).\\nThis has spurred interest in jointly modeling textual and struc-\\ntural data with GNNs and LMs. Notable architectures include (1)\\na cascading architecture that integrates GNNs with LM features\\n(cascading GNN-LMs); (2) nested GNN-LMs that blend message\\npassing into the language modeling process. Examples of the latter\\ninclude arXiv:2404.18271v1 [cs.CL] 28 Apr 2024\"\\\n",
    "\". answer this question: what is the purpose of this research paper?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25985834-f64a-43d0-8290-7472338a0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = \"\"\" You are a helpful assistant who tries to answer the question at the end using this information:  'Conference‚Äô17, July 2017, Washington, DC, USA Qi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun, and George Karypis\\n(2) In the same tables, we also find GNN (Sentence-BERT) per-\\nform worse than GNN (PEFT-LLaMA). In addition, GraphForm-\\ners also reports subpar performance against GNN (PEFT-LLaMA).\\nTherefore, we conclude that large language models serve as power-\\nful feature encoders for representation learning on text-rich graphs.\\nMeanwhile, InstructGLM outperforms GraphFormers but falls short\\nof PATTON‚Äôs performance. This discrepancy may be attributed to\\nthe challenges of handling graph structures through embedding-\\nbased in-context learning, which struggles with accurately cap-\\nturing structures via natural language descriptions. Consequently,\\ncarefully fine-tuned GNN-LMs ( GPEFT and PATTON) tend to sur-\\npass the embeddings produced by black-box LLMs.\\n(3)GPEFT consistently outperforms the compared baseline across\\nboth metrics. Specifically, it surpasses PEFT-LLaMA by 2.6% and\\n1.8% on MAG and Amazon Reviews, respectively, in terms of hit@1.\\nCompared to its own ablation, we find that pre-training helps im-\\nprove the accuracy of link prediction, especially in the academic\\ntext-rich graph.\\nRQ2: Is the pre-training of the graph prompt encoder neces-\\nsary?\\nOn MAG, we observe that our approach performs significantly\\nworse (‚àº6%) without pretraining compared to its pretrained coun-\\nterpart. On Amazon Review, while the performance gap is smaller\\nacross three subdomains, the Video Games category demonstrates\\ninstability without pretraining. Interestingly, Patton also outper-\\nforms GraphFormers with the same architecture on subdomains\\nthat utilize pre-trained checkpoints. Both methods introduce virtual\\nstructural tokens, and employing the same pre-training objective\\nas the pre-trained language model. Apparently, continual training\\nhelps align the virtual token representations with text representa-\\ntions. Therefore, we advocate for a pre-training then fine-tuning\\nparadigm, where one needs to pre-train only once in a domain and\\ncan fine-tune for various applications ( e.g., co-purchase, churn, etc.).\\n5.5 Model Analysis\\nIn this section, we provide more in-depth study to understand\\nperformance and efficiency of GPEFT .\\nRQ3:Can our approach be adapted to different PEFT methods\\nand LLMs?\\n(a) Varying PEFT algorithms\\n (b) Varying backbone LLMs\\nFigure 3: GPEFT using different LLMs and PEFT algorithms\\non Amazon Review.\\nWe first apply prefix tuning [ 24] onGPEFT and demonstrate the\\nresults on four subdomains of Amazon Review in Figure 3a. We\\ncan find that using prefix-tuning or LORA does not show muchTable 5: Time and memory costs of GPEFT on MAG-\\nEconomics using 8 A100 GPUs with a total batch size of 32.\\npre-training fine-tuning inference\\nTime 36min 96min 11min\\nMemory 20883MB 28309MB 13676MB\\n#trainable 5.5M 14.4M -\\ndifference, which indicates the versatility of our framework on\\nvarious PEFT techniques.\\nSecond, we substitute the backbone language model with OPT-\\n1.3B [ 47] and Falcon-7B2. In Figure 3b, we observe that, in general,\\nLLaMA-7B outperforms the others, with Falcon-7B in second place\\nand OPT-1.3B being outperformed by these two models. Our re-\\nsults are consistent with other work that shows LLMs with more\\nparameters yield better performance in downstream applications.\\nRQ4: Is GPEFT efficient in large-scale representation learn-\\ning?\\nIn Table 5, we report the running time of our approach at each\\nphase on MAG-Economics. Specifically, unlike the pre-training\\ntime of PATTON [ 19] (10hr+), the pre-training time of GPEFT is\\neven shorter than the fine-tuning time. Because we only optimize\\nŒòùëîin the pre-training phase and we can say that the pre-training\\nis both necessary and efficient together with RQ2 . Owing to its\\nparameter-efficient tuning, our approach not only minimizes the\\nnumber of training parameters but also achieves the best perfor-\\nmance. As mentioned in the introduction, the design of GPEFT\\naims to provide a high-quality, lightweight framework suitable for\\nvarious applications on different text-rich graphs. Considering the\\nreduced parameter storage requirements and the training time, we\\nbelieve GPEFT holds significant potential for industrial-scale ap-\\nplications, including recommendation systems, ranking tasks, and\\netc.\\nRQ5: What is the parameter sensitivity of GPEFT ?\\nThere are several important hyper-parameters in GPEFT : (1)\\nLORA rank ùëü, it affects the number of trainable parameters (2)\\nnumber of hops ùëòin GNN prompt encoder, it affects the amount of\\nstructural information used in the training of GPEFT . In Figure 4,\\nwe observe that varying the rank of LoRA matrices from 4 to 32 does\\nnot significantly affect the performance. This observation aligns\\nwith findings from other studies, which suggest that optimizing for\\na single task requires only minor adjustments to the pre-trained\\nLLM weights. Similarly, varying number of hops in the GNN prompt\\nencoder has a minor effect on performance, until the point where\\nmessage passing begins to aggregate more noisy neighbors than\\nuseful ones (e.g., at 4 hops, as shown in Figure 4b).\\n6 CONCLUSION AND FUTURE WORK\\nThis paper proposes GPEFT to harness LLMs for representation\\nlearning on text-rich graphs. Compared with existing work of ap-\\nplying LLM on graph structure data, our proposed method is the\\nfirst one that generates embeddings from the LLM and therefore can\\nbe applied on numerous industrial applications. More importantly,\\n2https://huggingface.co/tiiuae/falcon-7b', metadata={'source': './prof_1/2404.18271v1.pdf', 'page': 7})]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60f57fb7-a23d-42a7-968b-ce43fe307530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8ee0dd9-cad7-46c7-97f0-57011775abb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9a5ee030314df6b5fd4218e828b325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/860 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ad8860843e4456b80b9772017da520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66dc8ca01124f39856f83c6837d41e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ad4c2b0dd04a1ea8227af8321a21fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b57d5691c4148758bf121153e8b90c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533dc1de32854ecfaa7555930ef23f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ec73d272074c9897515fa073230dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"MBZUAI/LaMini-Flan-T5-783M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3170d351-8fb4-4dff-921d-79a477fdea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayyss/.conda/envs/RAG/lib/python3.12/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The text that provides data for modeling is unanswerable as the provided information is a'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(rag, truncation=True, do_sample=True)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1aba55d-fa88-49e9-a052-56ee3e9c42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v0.4\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c2d6120-d383-4939-a7d2-074147fb208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag += \". answer this question: what is the purpose of this research paper?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0925820-5a61-406a-b442-75162249e5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" You are a helpful assistant who tries to answer the question at the end using this information:  'Conference‚Äô17, July 2017, Washington, DC, USA Qi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun, and George Karypis\\n(2) In the same tables, we also find GNN (Sentence-BERT) per-\\nform worse than GNN (PEFT-LLaMA). In addition, GraphForm-\\ners also reports subpar performance against GNN (PEFT-LLaMA).\\nTherefore, we conclude that large language models serve as power-\\nful feature encoders for representation learning on text-rich graphs.\\nMeanwhile, InstructGLM outperforms GraphFormers but falls short\\nof PATTON‚Äôs performance. This discrepancy may be attributed to\\nthe challenges of handling graph structures through embedding-\\nbased in-context learning, which struggles with accurately cap-\\nturing structures via natural language descriptions. Consequently,\\ncarefully fine-tuned GNN-LMs ( GPEFT and PATTON) tend to sur-\\npass the embeddings produced by black-box LLMs.\\n(3)GPEFT consistently outperforms the compared baseline across\\nboth metrics. Specifically, it surpasses PEFT-LLaMA by 2.6% and\\n1.8% on MAG and Amazon Reviews, respectively, in terms of hit@1.\\nCompared to its own ablation, we find that pre-training helps im-\\nprove the accuracy of link prediction, especially in the academic\\ntext-rich graph.\\nRQ2: Is the pre-training of the graph prompt encoder neces-\\nsary?\\nOn MAG, we observe that our approach performs significantly\\nworse (‚àº6%) without pretraining compared to its pretrained coun-\\nterpart. On Amazon Review, while the performance gap is smaller\\nacross three subdomains, the Video Games category demonstrates\\ninstability without pretraining. Interestingly, Patton also outper-\\nforms GraphFormers with the same architecture on subdomains\\nthat utilize pre-trained checkpoints. Both methods introduce virtual\\nstructural tokens, and employing the same pre-training objective\\nas the pre-trained language model. Apparently, continual training\\nhelps align the virtual token representations with text representa-\\ntions. Therefore, we advocate for a pre-training then fine-tuning\\nparadigm, where one needs to pre-train only once in a domain and\\ncan fine-tune for various applications ( e.g., co-purchase, churn, etc.).\\n5.5 Model Analysis\\nIn this section, we provide more in-depth study to understand\\nperformance and efficiency of GPEFT .\\nRQ3:Can our approach be adapted to different PEFT methods\\nand LLMs?\\n(a) Varying PEFT algorithms\\n (b) Varying backbone LLMs\\nFigure 3: GPEFT using different LLMs and PEFT algorithms\\non Amazon Review.\\nWe first apply prefix tuning [ 24] onGPEFT and demonstrate the\\nresults on four subdomains of Amazon Review in Figure 3a. We\\ncan find that using prefix-tuning or LORA does not show muchTable 5: Time and memory costs of GPEFT on MAG-\\nEconomics using 8 A100 GPUs with a total batch size of 32.\\npre-training fine-tuning inference\\nTime 36min 96min 11min\\nMemory 20883MB 28309MB 13676MB\\n#trainable 5.5M 14.4M -\\ndifference, which indicates the versatility of our framework on\\nvarious PEFT techniques.\\nSecond, we substitute the backbone language model with OPT-\\n1.3B [ 47] and Falcon-7B2. In Figure 3b, we observe that, in general,\\nLLaMA-7B outperforms the others, with Falcon-7B in second place\\nand OPT-1.3B being outperformed by these two models. Our re-\\nsults are consistent with other work that shows LLMs with more\\nparameters yield better performance in downstream applications.\\nRQ4: Is GPEFT efficient in large-scale representation learn-\\ning?\\nIn Table 5, we report the running time of our approach at each\\nphase on MAG-Economics. Specifically, unlike the pre-training\\ntime of PATTON [ 19] (10hr+), the pre-training time of GPEFT is\\neven shorter than the fine-tuning time. Because we only optimize\\nŒòùëîin the pre-training phase and we can say that the pre-training\\nis both necessary and efficient together with RQ2 . Owing to its\\nparameter-efficient tuning, our approach not only minimizes the\\nnumber of training parameters but also achieves the best perfor-\\nmance. As mentioned in the introduction, the design of GPEFT\\naims to provide a high-quality, lightweight framework suitable for\\nvarious applications on different text-rich graphs. Considering the\\nreduced parameter storage requirements and the training time, we\\nbelieve GPEFT holds significant potential for industrial-scale ap-\\nplications, including recommendation systems, ranking tasks, and\\netc.\\nRQ5: What is the parameter sensitivity of GPEFT ?\\nThere are several important hyper-parameters in GPEFT : (1)\\nLORA rank ùëü, it affects the number of trainable parameters (2)\\nnumber of hops ùëòin GNN prompt encoder, it affects the amount of\\nstructural information used in the training of GPEFT . In Figure 4,\\nwe observe that varying the rank of LoRA matrices from 4 to 32 does\\nnot significantly affect the performance. This observation aligns\\nwith findings from other studies, which suggest that optimizing for\\na single task requires only minor adjustments to the pre-trained\\nLLM weights. Similarly, varying number of hops in the GNN prompt\\nencoder has a minor effect on performance, until the point where\\nmessage passing begins to aggregate more noisy neighbors than\\nuseful ones (e.g., at 4 hops, as shown in Figure 4b).\\n6 CONCLUSION AND FUTURE WORK\\nThis paper proposes GPEFT to harness LLMs for representation\\nlearning on text-rich graphs. Compared with existing work of ap-\\nplying LLM on graph structure data, our proposed method is the\\nfirst one that generates embeddings from the LLM and therefore can\\nbe applied on numerous industrial applications. More importantly,\\n2https://huggingface.co/tiiuae/falcon-7b', metadata={'source': './prof_1/2404.18271v1.pdf', 'page': 7})]. answer this question: what is the purpose of this research paper?\\nis our work the first to introduce a language model for large-scale\\nrepresentation learning on graphs, which is a well-defined research\\ntopic\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(rag, truncation=True, do_sample=True)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d2342fe-cefc-48b4-84f0-e3797cad2a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"MBZUAI/LaMini-Neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5a5f3a1-6da3-47a4-848b-50112212551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=32) and `max_length`(=1500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" You are a helpful assistant who tries to answer the question at the end using this information:  'Conference‚Äô17, July 2017, Washington, DC, USA Qi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun, and George Karypis\\n(2) In the same tables, we also find GNN (Sentence-BERT) per-\\nform worse than GNN (PEFT-LLaMA). In addition, GraphForm-\\ners also reports subpar performance against GNN (PEFT-LLaMA).\\nTherefore, we conclude that large language models serve as power-\\nful feature encoders for representation learning on text-rich graphs.\\nMeanwhile, InstructGLM outperforms GraphFormers but falls short\\nof PATTON‚Äôs performance. This discrepancy may be attributed to\\nthe challenges of handling graph structures through embedding-\\nbased in-context learning, which struggles with accurately cap-\\nturing structures via natural language descriptions. Consequently,\\ncarefully fine-tuned GNN-LMs ( GPEFT and PATTON) tend to sur-\\npass the embeddings produced by black-box LLMs.\\n(3)GPEFT consistently outperforms the compared baseline across\\nboth metrics. Specifically, it surpasses PEFT-LLaMA by 2.6% and\\n1.8% on MAG and Amazon Reviews, respectively, in terms of hit@1.\\nCompared to its own ablation, we find that pre-training helps im-\\nprove the accuracy of link prediction, especially in the academic\\ntext-rich graph.\\nRQ2: Is the pre-training of the graph prompt encoder neces-\\nsary?\\nOn MAG, we observe that our approach performs significantly\\nworse (‚àº6%) without pretraining compared to its pretrained coun-\\nterpart. On Amazon Review, while the performance gap is smaller\\nacross three subdomains, the Video Games category demonstrates\\ninstability without pretraining. Interestingly, Patton also outper-\\nforms GraphFormers with the same architecture on subdomains\\nthat utilize pre-trained checkpoints. Both methods introduce virtual\\nstructural tokens, and employing the same pre-training objective\\nas the pre-trained language model. Apparently, continual training\\nhelps align the virtual token representations with text representa-\\ntions. Therefore, we advocate for a pre-training then fine-tuning\\nparadigm, where one needs to pre-train only once in a domain and\\ncan fine-tune for various applications ( e.g., co-purchase, churn, etc.).\\n5.5 Model Analysis\\nIn this section, we provide more in-depth study to understand\\nperformance and efficiency of GPEFT .\\nRQ3:Can our approach be adapted to different PEFT methods\\nand LLMs?\\n(a) Varying PEFT algorithms\\n (b) Varying backbone LLMs\\nFigure 3: GPEFT using different LLMs and PEFT algorithms\\non Amazon Review.\\nWe first apply prefix tuning [ 24] onGPEFT and demonstrate the\\nresults on four subdomains of Amazon Review in Figure 3a. We\\ncan find that using prefix-tuning or LORA does not show muchTable 5: Time and memory costs of GPEFT on MAG-\\nEconomics using 8 A100 GPUs with a total batch size of 32.\\npre-training fine-tuning inference\\nTime 36min 96min 11min\\nMemory 20883MB 28309MB 13676MB\\n#trainable 5.5M 14.4M -\\ndifference, which indicates the versatility of our framework on\\nvarious PEFT techniques.\\nSecond, we substitute the backbone language model with OPT-\\n1.3B [ 47] and Falcon-7B2. In Figure 3b, we observe that, in general,\\nLLaMA-7B outperforms the others, with Falcon-7B in second place\\nand OPT-1.3B being outperformed by these two models. Our re-\\nsults are consistent with other work that shows LLMs with more\\nparameters yield better performance in downstream applications.\\nRQ4: Is GPEFT efficient in large-scale representation learn-\\ning?\\nIn Table 5, we report the running time of our approach at each\\nphase on MAG-Economics. Specifically, unlike the pre-training\\ntime of PATTON [ 19] (10hr+), the pre-training time of GPEFT is\\neven shorter than the fine-tuning time. Because we only optimize\\nŒòùëîin the pre-training phase and we can say that the pre-training\\nis both necessary and efficient together with RQ2 . Owing to its\\nparameter-efficient tuning, our approach not only minimizes the\\nnumber of training parameters but also achieves the best perfor-\\nmance. As mentioned in the introduction, the design of GPEFT\\naims to provide a high-quality, lightweight framework suitable for\\nvarious applications on different text-rich graphs. Considering the\\nreduced parameter storage requirements and the training time, we\\nbelieve GPEFT holds significant potential for industrial-scale ap-\\nplications, including recommendation systems, ranking tasks, and\\netc.\\nRQ5: What is the parameter sensitivity of GPEFT ?\\nThere are several important hyper-parameters in GPEFT : (1)\\nLORA rank ùëü, it affects the number of trainable parameters (2)\\nnumber of hops ùëòin GNN prompt encoder, it affects the amount of\\nstructural information used in the training of GPEFT . In Figure 4,\\nwe observe that varying the rank of LoRA matrices from 4 to 32 does\\nnot significantly affect the performance. This observation aligns\\nwith findings from other studies, which suggest that optimizing for\\na single task requires only minor adjustments to the pre-trained\\nLLM weights. Similarly, varying number of hops in the GNN prompt\\nencoder has a minor effect on performance, until the point where\\nmessage passing begins to aggregate more noisy neighbors than\\nuseful ones (e.g., at 4 hops, as shown in Figure 4b).\\n6 CONCLUSION AND FUTURE WORK\\nThis paper proposes GPEFT to harness LLMs for representation\\nlearning on text-rich graphs. Compared with existing work of ap-\\nplying LLM on graph structure data, our proposed method is the\\nfirst one that generates embeddings from the LLM and therefore can\\nbe applied on numerous industrial applications. More importantly,\\n2https://huggingface.co/tiiuae/falcon-7b', metadata={'source': './prof_1/2404.18271v1.pdf', 'page': 7})]end of information. question: what is a GNN? on graphs. Our key contributions can be summarized as\\n1. We demonstrate the effectiveness of lightweight graph\\nprompt encoders (GPE\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(rag, max_length=1500, truncation=True, do_sample=True)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82bf6455-ee98-4448-b893-cb841de26a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from pydantic import Field\n",
    "\n",
    "class T5Embeddings(Embeddings):\n",
    "\n",
    "    def __init__(self, model_name: str = \"MBZUAI/LaMini-T5-738M\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _embed_text(self, text: str) -> List[float]:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs.input_ids.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model.shared(input_ids).mean(dim=1)\n",
    "        \n",
    "        return embeddings.squeeze().cpu().numpy().tolist()\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        return [self._embed_text(text) for text in texts]\n",
    "\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        return self._embed_text(text)\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    n: int\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "            \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = input_ids.input_ids.to(self.device)\n",
    "        \n",
    "        outputs = model.generate(input_ids, max_length=512)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622f93ba-3fa2-4e97-b13e-6cdfc24ddbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v0.4\", device=0)\n",
    "\n",
    "class CustomLLM_tiny(LLM):\n",
    "    n: int\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\\\n",
    "\n",
    "        return pipe(prompt, do_sample=True)[0]['generated_text']\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4962b05e-944b-4338-8676-23c50916d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_pdf(pdf_folder_path, embeddings):\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "            \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    faiss_index = FAISS.from_documents(chunked_documents, embeddings)\n",
    "    \n",
    "    return faiss_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759e0e8-80c5-4378-97be-314ef5c3fef1",
   "metadata": {},
   "source": [
    "### TinyLlama with llama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "704d9b90-b085-4bf6-acab-c34530f22d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "llm = CustomLLM_tiny(n=5)\n",
    "embeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")\n",
    "\n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b26b5c95-873b-49c2-85f3-89e063f82c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
      "Context: Conference‚Äô17, July 2017, Washington, DC, USA Qi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun, and George Karypis\n",
      "Neural Information Processing Systems 33 (2020), 16857‚Äì16867.\n",
      "[41] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang,\n",
      "Nitesh V Chawla, and Panpan Xu. 2023. Graph Neural Prompting with Large\n",
      "Language Models. arXiv preprint arXiv:2309.15427 (2023).\n",
      "[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\n",
      "Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\n",
      "Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv\n",
      "preprint arXiv:2302.13971 (2023).\n",
      "[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n",
      "Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. Advances inneural information processing systems 30 (2017).\n",
      "[44] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal,\n",
      "Amit Singh, Guangzhong Sun, and Xing Xie. 2021. GraphFormers: GNN-nested\n",
      "transformers for representation learning on textual graph. Advances inNeural\n",
      "Information Processing Systems 34 (2021), 28798‚Äì28810.\n",
      "[45] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. 2023.\n",
      "Natural language is all a graph needs. arXiv preprint arXiv:2308.07134 (2023).\n",
      "[46] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V\n",
      "Chawla. 2019. Heterogeneous graph neural network. In Proceedings ofthe\n",
      "25th ACM SIGKDD International Conference onKnowledge Discovery &Data\n",
      "Mining. 793‚Äì803.\n",
      "[47] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\n",
      "Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al .2022. Opt:\n",
      "Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068\n",
      "(2022).\n",
      "[48] Yu Zhang, Bowen Jin, Qi Zhu, Yu Meng, and Jiawei Han. 2023. The effect of\n",
      "metadata on scientific literature tagging: A cross-field cross-model study. In\n",
      "WWW‚Äô23. 1626‚Äì1637.\n",
      "[49] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and\n",
      "Jian Tang. 2022. Learning on large-scale text-attributed graphs via variational\n",
      "inference. arXiv preprint arXiv:2210.14709 (2022).\n",
      "[50] Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [mask]:\n",
      "Learning vs. learning to recall. arXiv preprint arXiv:2104.05240 (2021).\n",
      "\n",
      "12 Costas Mavromatis, Petros Karypis, and George Karypis\n",
      "17. Mavromatis, C., Karypis, G.: Rearev: Adaptive reasoning for question answering\n",
      "over knowledge graphs. In: EMNLP Findings (2022)\n",
      "18. Mihaylov, T., Clark, P., Khot, T., Sabharwal, A.: Can a suit of armor conduct\n",
      "electricity? a new dataset for open book question answering. In: EMNLP (2018)\n",
      "19. Mihaylov, T., Frank, A.: Knowledgeable reader: Enhancing cloze-style reading com-\n",
      "prehension with external commonsense knowledge. In: ACL (2018)\n",
      "20. Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., Wu, X.: Unifying large language\n",
      "models and knowledge graphs: A roadmap. arXiv preprint arXiv:2306.08302 (2023)\n",
      "21. Park, J., Choi, H.K., Ko, J., Park, H., Kim, J.H., Jeong, J., Kim, K., Kim,\n",
      "H.: Relation-aware language-graph transformer for question answering. In: AAAI\n",
      "(2023)\n",
      "22. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\n",
      "W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text\n",
      "transformer. JMLR (2020)\n",
      "23. Schlichtkrull, M., Kipf, T.N., Bloem, P., Van Den Berg, R., Titov, I., Welling, M.:\n",
      "Modeling relational data with graph convolutional networks. In: ESWC (2018)\n",
      "24. Speer, R., Chin, J., Havasi, C.: Conceptnet 5.5: An open multilingual graph of\n",
      "general knowledge. In: AAAI (2017)\n",
      "25. Sun, Y., Shi, Q., Qi, L., Zhang, Y.: Jointlk: Joint reasoning with language mod-\n",
      "els and knowledge graphs for commonsense question answering. In: NAACL-HLT\n",
      "(2022)\n",
      "26. Tian, Y., Song, H., Wang, Z., Wang, H., Hu, Z., Wang, F., Chawla, N.V.,\n",
      "Xu, P.: Graph neural prompting with large language models. arXiv preprint\n",
      "arXiv:2309.15427 (2023)\n",
      "27. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n",
      " L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)\n",
      "28. VeliÀá ckovi¬¥ c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y.: Graph\n",
      "attention networks. In: ICLR (2018)\n",
      "29. Wang, K., Zhang, Y., Yang, D., Song, L., Qin, T.: Gnn is a counter? revisiting gnn\n",
      "for question answering. In: ICLR (2022)\n",
      "30. Xie, H., Zheng, D., Ma, J., Zhang, H., Ioannidis, V.N., Song, X., Ping, Q., Wang,\n",
      "S., Yang, C., Xu, Y., et al.: Graph-aware language model pre-training on a large\n",
      "graph corpus can help multiple graph applications (2023)\n",
      "31. Xie, T., Wu, C.H., Shi, P., Zhong, R., Scholak, T., Yasunaga, M., Wu, C.S., Zhong,\n",
      "M., Yin, P., Wang, S.I., et al.: Unifiedskg: Unifying and multi-tasking structured\n",
      "knowledge grounding with text-to-text language models. In: EMNLP (2022)\n",
      "32. Yasunaga, M., Bosselut, A., Ren, H., Zhang, X., Manning, C.D., Liang,\n",
      "P., Leskovec, J.: Deep bidirectional language-knowledge graph pretraining. In:\n",
      "NeurIPS (2022)\n",
      "33. Yasunaga, M., Leskovec, J., Liang, P.: Linkbert: Pretraining language models with\n",
      "document links. In: ACL (2022)\n",
      "34. Yasunaga, M., Ren, H., Bosselut, A., Liang, P., Leskovec, J.: Qa-gnn: Reasoning\n",
      "with language models and knowledge graphs for question answering. In: NAACL\n",
      "(2021)\n",
      "35. Zhang, X., Bosselut, A., Yasunaga, M., Ren, H., Liang, P., Manning, C.D.,\n",
      "Leskovec, J.: Greaselm: Graph reasoning enhanced language models for question\n",
      "answering. In: ICLR (2022)\n",
      "36. Zhao, J., Qu, M., Li, C., Yan, H., Liu, Q., Li, R., Xie, X., Tang, J.: Learning on\n",
      "large-scale text-attributed graphs via variational inference. In: ICLR (2023)\n",
      "37. Zhu, Z., Galkin, M., Zhang, Z., Tang, J.: Neural-symbolic models for logical queries\n",
      "on knowledge graphs. In: ICML (2022)\n",
      "\n",
      "SemPool: KG pooling for enhancing language models 11\n",
      "Corp, and Amazon Web Services. Access to research and computing facilities\n",
      "was provided by the Minnesota Supercomputing Institute.\n",
      "References\n",
      "1. Agarwal, O., Ge, H., Shakeri, S., Al-Rfou, R.: Knowledge graph based synthetic cor-\n",
      "pus generation for knowledge-enhanced language model pre-training. In: NAACL\n",
      "(2021)\n",
      "2. Clark, P., Etzioni, O., Khot, T., Khashabi, D., Mishra, B., Richardson, K., Sab-\n",
      "harwal, A., Schoenick, C., Tafjord, O., Tandon, N., et al.: From ‚Äòf‚Äôto ‚Äòa‚Äôon the ny\n",
      "regents science exams: An overview of the aristo project. AI Magazine (2020)\n",
      "3. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\n",
      "tional transformers for language understanding. arXiv preprint arXiv:1810.04805\n",
      "(2018)\n",
      "4. Feng, Y., Chen, X., Lin, B.Y., Wang, P., Yan, J., Ren, X.: Scalable multi-hop\n",
      "relational reasoning for knowledge-aware question answering. In: EMNLP (2020)\n",
      "5. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message\n",
      "passing for quantum chemistry. In: ICML (2017)\n",
      "6. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\n",
      "In: IEEE CVPR (2016)\n",
      "7. Ioannidis, V.N., Song, X., Zheng, D., Zhang, H., Ma, J., Xu, Y., Zeng, B., Chilimbi,\n",
      "T., Karypis, G.: Efficient and effective training of language and graph neural net-\n",
      "work models. arXiv preprint arXiv:2206.10781 (2022)\n",
      "8. Jin, D., Pan, E., Oufattole, N., Weng, W.H., Fang, H., Szolovits, P.: What disease\n",
      "does this patient have? a large-scale open domain question answering dataset from\n",
      "medical exams. Applied Sciences (2021)\n",
      "9. Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., Ha-\n",
      "jishirzi, H.: UNIFIEDQA: Crossing format boundaries with a single QA system.\n",
      "In: EMNLP Findings (2020)\n",
      "10. Lin, B.Y., Chen, X., Chen, J., Ren, X.: Kagnet: Knowledge-aware graph networks\n",
      "for commonsense reasoning. In: EMNLP-IJCNLP (2019)\n",
      "11. Lin, B.Y., Wu, Z., Yang, Y., Lee, D.H., Ren, X.: Riddlesense: Reasoning about\n",
      "riddle questions featuring linguistic creativity and commonsense knowledge. In:\n",
      "ACL Findings (2021)\n",
      "12. Liu, F., Shareghi, E., Meng, Z., Basaldella, M., Collier, N.: Self-alignment pretrain-\n",
      "ing for biomedical entity representations. In: NAACL-HLT (2021)\n",
      "13. Liu, J., Yang, C., Lu, Z., Chen, J., Li, Y., Zhang, M., Bai, T., Fang, Y., Sun, L.,\n",
      "Yu, P.S., et al.: Towards graph foundation models: A survey and beyond. arXiv\n",
      "preprint arXiv:2310.11829 (2023)\n",
      "14. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\n",
      "Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\n",
      "approach. arXiv preprint arXiv:1907.11692 (2019)\n",
      "15. Mallen, A., Asai, A., Zhong, V., Das, R., Hajishirzi, H., Khashabi, D.: When not\n",
      "to trust language models: Investigating effectiveness and limitations of parametric\n",
      "and non-parametric memories. In: ACL (2023)\n",
      "16. Mavromatis, C., Ioannidis, V.N., Wang, S., Zheng, D., Adeshina, S., Ma, J., Zhao,\n",
      "H., Faloutsos, C., Karypis, G.: Train your own gnn teacher: Graph-aware distilla-\n",
      "tion on textual graphs. In: ECML-PKDD (2023)\n",
      "\n",
      "2 Costas Mavromatis, Petros Karypis, and George Karypis\n",
      "Fine-tuned\n",
      "LM[Graph] [CLS] [When] [birds] .....(winter , causes, migration)\n",
      "(birds, capable of, flying)\n",
      ".\n",
      ".\n",
      "(south, is, warm)KG subgraph facts\n",
      "Pre-trained\n",
      "LM\n",
      "Semantic\n",
      "Embeddings\n",
      "Pooling\n",
      "(self-att.) ‚ùÑ \n",
      "`When birds migrate south for\n",
      "the winter , they  do it because\n",
      "[they are genetically called to]'üî•\n",
      "Correct\n",
      "‚úÖSemPool\n",
      "[Graph] [CLS] ...Classification subgraph\n",
      "retrievalKG groundingVerbalizationPrecomputed\n",
      "Fig. 1: Our SemPool method performs simple graph pooling to enhance the LM‚Äôs\n",
      "reasoning. Facts of the KG are represented by their semantic information with\n",
      "pre-trained LMs. SemPool aggregates the graph‚Äôs semantic information into a\n",
      "single representation that is fed into the LM for QA.\n",
      "Nevertheless, GNNs operate on graph data while LMs use natural language\n",
      "sequences, which makes information exchange between the two modalities chal-\n",
      "lenging. In fact, our empirical findings (Section 4) suggest that GNNs mainly\n",
      "provide graph statistical information for the QA task [29] rather than informa-\n",
      "tion that grounds the LM‚Äôs reasoning and is robust under graph perturbations.\n",
      "In addition, the representation space mismatch between graph (KGs are usually\n",
      "represented with external node embeddings) and language (represented with pre-\n",
      "trained LMs) does not aid the information exchange between the two modalities.\n",
      "In this work, we present SemPool, a simple graph pooling method that en-\n",
      "hances the LM‚Äôs reasoning with KG textual information. As illustrated in Fig-\n",
      "ure 1, SemPool represents each fact in the KG with the pre-trained LM, aiming\n",
      "at semantic alignment between graph and language. SemPool then performs a\n",
      "global graph pooling operation in order to aggregate semantic information from\n",
      "the whole graph into a single representation. The aggregated representation is\n",
      "fused as input into the fine-tuned LM for QA, which grounds the LM‚Äôs reasoning\n",
      "to the information provided. Moreover, we extend SemPool to fuse different type\n",
      "of semantic information into different LM‚Äôs layers (Section 5.3), providing more\n",
      "flexibility during learning.\n",
      "SemPool demonstrates robust performance under different settings. We ex-\n",
      "periment with standard QA benchmarks (OpenbookQA, RiddleSense, MedQA-\n",
      "USMLE), (i) when complemented by complete in-domain KGs, and (ii) when\n",
      "complemented by in-domain KGs where critical information about the candi-\n",
      "date answers is missing. SemPool outperforms the bestperforming GNN-based\n",
      "approach by 2.27% accuracy points in the challenging case, while it is compet-\n",
      "itive (second-best) in the easier case. In addition, our experiments show that\n",
      "SemPool is effective under different LMs (Section 7.1), highlight the importance\n",
      "of semantic alignment between language and graph, and illustrate SemPool‚Äôs\n",
      "interpretability (Section 7.2).\n",
      "\n",
      "Question: what is a graph neural network (GNN)?\n",
      "\n",
      "Answer: Ni NE ba Sep MA Ana MS B Du Han Cel L D B Bah Bah0 D Lu Li AA B Lu CS Luc Sat NE Ba DE LP Cy Au\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"what is a graph neural network (GNN)?\"\n",
    "result = qa_chain({\"queary\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb8a22-59de-4501-acda-56a5385534d2",
   "metadata": {},
   "source": [
    "### T5 model with T5 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1e551ac-43f3-453c-82d5-553a5fc171d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "embeddings = T5Embeddings(\"MBZUAI/LaMini-T5-738M\") \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 1})\n",
    "llm = CustomLLM(n=5)\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2230001c-01ef-4381-bf96-4d2d8252ab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "embeddings = T5Embeddings(\"MBZUAI/LaMini-T5-738M\") \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "000d82dc-c589-40e9-8343-2617427b9ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Parameter-Efficient Tuning Large Language Models for Graph\\nRepresentation Learning\\nQi Zhu\\nAmazon Web Services\\nqzhuamzn@amazon.comDa Zheng\\nAmazon Web Services\\ndzzhen@amazon.comXiang Song\\nAmazon Web Services\\ndzzhen@amazon.com\\nShichang Zhang‚àó\\nUniversity of California, Los Angeles\\nshichang@cs.ucla.eduBowen Jin‚àó\\nUniversity of Illinois\\nUrbana-Champaign\\nbowenj4@illinois.eduYizhou Sun\\nUniversity of California, Los Angeles\\nyzsun@cs.ucla.edu\\nGeorge Karypis\\nAmazon Web Services\\ngkarypis@amazon.com\\nABSTRACT\\nText-rich graphs, which exhibit rich textual information on nodes\\nand edges, are prevalent across a wide range of real-world business\\napplications. Large Language Models (LLMs) have demonstrated re-\\nmarkable abilities in understanding text, which also introduced the\\npotential for more expressive modeling in text-rich graphs. Despite\\nthese capabilities, efficiently applying LLMs to representation learn-\\ning on graphs presents significant challenges. Recently, parameter-\\nefficient fine-tuning methods for LLMs have enabled efficient new\\ntask generalization with minimal time and memory consumption.\\nInspired by this, we introduce Graph-aware Parameter-Efficient\\nFine-Tuning - GPEFT , a novel approach for efficient graph repre-\\nsentation learning with LLMs on text-rich graphs. Specifically, we\\nutilize a graph neural network (GNN) to encode structural infor-\\nmation from neighboring nodes into a graph prompt. This prompt\\nis then inserted at the beginning of the text sequence. To improve\\nthe quality of graph prompts, we pre-trained the GNN to assist the\\nfrozen LLM in predicting the next token in the node text. Compared\\nwith existing joint GNN and LMs, our method directly generate the\\nnode embeddings from large language models with an affordable\\nfine-tuning cost. We validate our approach through comprehen-\\nsive experiments conducted on 8 different text-rich graphs, observ-\\ning an average improvement of 2% in hit@1 and Mean Reciprocal\\nRank (MRR) in link prediction evaluations. Our results demonstrate\\nthe efficacy and efficiency of our model, showing that it can be\\nsmoothly integrated with various large language models, including\\nOPT, LLaMA and Falcon.\\n‚àóWork done while being an intern at Amazon.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nConference‚Äô17, July 2017, Washington, DC, USA\\n¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnKEYWORDS\\nRepresentation Learning, Graphs, Language Models\\nACM Reference Format:\\nQi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun,\\nand George Karypis. 2024. Parameter-Efficient Tuning Large Language\\nModels for Graph Representation Learning. In .ACM, New York, NY, USA,\\n11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nInformation networks form the backbone of modern data systems:\\nmillions of daily social posts are shared among users on platforms\\nlike Facebook and Twitter; countless papers are published and cited\\nwithin academic networks. Many of these networks are rich in\\ntextual information on various types of objects, known as text-rich\\nor text-attributed networks. For example, in an e-commerce graph,\\ntext-rich information could be used to predict links by analyzing\\nproduct descriptions, user reviews, and metadata to recommend\\nitems that are frequently bought together. Such real-world applica-\\ntions typify the problem of link prediction, where representation\\nlearning ( i.e., embedding) emerges as the most prevalent solution.\\nRepresentation learning on graph-structured data aims to learn\\na dense vector for each node through self-supervised tasks such as\\nlink prediction [ 9,34,46] or masked attribute recovery [ 14]. These\\ndense vectors can be widely utilized in various downstream ap-\\nplications including search, ranking and retrieval. To effectively\\nutilize both node attributes and structural information in represen-\\ntation learning, Graph Neural Networks (GNNs) [ 11,21] devise a\\nnovel type of neural networks with message-passing mechanism.\\nIn text-rich graphs, raw text is usually transformed into feature\\nvectors by a text encoder prior to the application of Graph Neu-\\nral Networks (GNNs). Early text encoders, such as bag-of-words\\nand Word2Vec[ 32] , are being gradually phased out due to signifi-\\ncant improvements in text representation from transformer-based\\nlanguage models (LMs).\\nThis has spurred interest in jointly modeling textual and struc-\\ntural data with GNNs and LMs. Notable architectures include (1)\\na cascading architecture that integrates GNNs with LM features\\n(cascading GNN-LMs); (2) nested GNN-LMs that blend message\\npassing into the language modeling process. Examples of the latterarXiv:2404.18271v1  [cs.CL]  28 Apr 2024', metadata={'source': './prof_1/2404.18271v1.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodfes into a graph prompt?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "990f16ee-655a-41b7-9826-8a31f5e6b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not provide information about which model is more efficient in predicting the next token in the node text.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"T5 MODEL WITH T5 EMBEDDINGS\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodfes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51de014-c49d-4eef-9ac2-c3f6546b31f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b797c477-84b7-4d38-a8c2-4d70ac1e4a91",
   "metadata": {},
   "source": [
    "### T5 model with llama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2931e4-e299-43b2-beec-18efdf7b49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")\n",
    "llm = CustomLLM(n=5)\n",
    "\n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7b216-f80d-4a93-a044-f32e787f71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"T5 MODEL WITH LLAMA EMBEDDINGS\"\n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74aa3843-3c7c-4500-b092-090d52064606",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dba64d0-1154-4a20-b949-f38fab6ada4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Conference‚Äô17, July 2017, Washington, DC, USA Qi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun, and George Karypis\\n(2) In the same tables, we also find GNN (Sentence-BERT) per-\\nform worse than GNN (PEFT-LLaMA). In addition, GraphForm-\\ners also reports subpar performance against GNN (PEFT-LLaMA).\\nTherefore, we conclude that large language models serve as power-\\nful feature encoders for representation learning on text-rich graphs.\\nMeanwhile, InstructGLM outperforms GraphFormers but falls short\\nof PATTON‚Äôs performance. This discrepancy may be attributed to\\nthe challenges of handling graph structures through embedding-\\nbased in-context learning, which struggles with accurately cap-\\nturing structures via natural language descriptions. Consequently,\\ncarefully fine-tuned GNN-LMs ( GPEFT and PATTON) tend to sur-\\npass the embeddings produced by black-box LLMs.\\n(3)GPEFT consistently outperforms the compared baseline across\\nboth metrics. Specifically, it surpasses PEFT-LLaMA by 2.6% and\\n1.8% on MAG and Amazon Reviews, respectively, in terms of hit@1.\\nCompared to its own ablation, we find that pre-training helps im-\\nprove the accuracy of link prediction, especially in the academic\\ntext-rich graph.\\nRQ2: Is the pre-training of the graph prompt encoder neces-\\nsary?\\nOn MAG, we observe that our approach performs significantly\\nworse (‚àº6%) without pretraining compared to its pretrained coun-\\nterpart. On Amazon Review, while the performance gap is smaller\\nacross three subdomains, the Video Games category demonstrates\\ninstability without pretraining. Interestingly, Patton also outper-\\nforms GraphFormers with the same architecture on subdomains\\nthat utilize pre-trained checkpoints. Both methods introduce virtual\\nstructural tokens, and employing the same pre-training objective\\nas the pre-trained language model. Apparently, continual training\\nhelps align the virtual token representations with text representa-\\ntions. Therefore, we advocate for a pre-training then fine-tuning\\nparadigm, where one needs to pre-train only once in a domain and\\ncan fine-tune for various applications ( e.g., co-purchase, churn, etc.).\\n5.5 Model Analysis\\nIn this section, we provide more in-depth study to understand\\nperformance and efficiency of GPEFT .\\nRQ3:Can our approach be adapted to different PEFT methods\\nand LLMs?\\n(a) Varying PEFT algorithms\\n (b) Varying backbone LLMs\\nFigure 3: GPEFT using different LLMs and PEFT algorithms\\non Amazon Review.\\nWe first apply prefix tuning [ 24] onGPEFT and demonstrate the\\nresults on four subdomains of Amazon Review in Figure 3a. We\\ncan find that using prefix-tuning or LORA does not show muchTable 5: Time and memory costs of GPEFT on MAG-\\nEconomics using 8 A100 GPUs with a total batch size of 32.\\npre-training fine-tuning inference\\nTime 36min 96min 11min\\nMemory 20883MB 28309MB 13676MB\\n#trainable 5.5M 14.4M -\\ndifference, which indicates the versatility of our framework on\\nvarious PEFT techniques.\\nSecond, we substitute the backbone language model with OPT-\\n1.3B [ 47] and Falcon-7B2. In Figure 3b, we observe that, in general,\\nLLaMA-7B outperforms the others, with Falcon-7B in second place\\nand OPT-1.3B being outperformed by these two models. Our re-\\nsults are consistent with other work that shows LLMs with more\\nparameters yield better performance in downstream applications.\\nRQ4: Is GPEFT efficient in large-scale representation learn-\\ning?\\nIn Table 5, we report the running time of our approach at each\\nphase on MAG-Economics. Specifically, unlike the pre-training\\ntime of PATTON [ 19] (10hr+), the pre-training time of GPEFT is\\neven shorter than the fine-tuning time. Because we only optimize\\nŒòùëîin the pre-training phase and we can say that the pre-training\\nis both necessary and efficient together with RQ2 . Owing to its\\nparameter-efficient tuning, our approach not only minimizes the\\nnumber of training parameters but also achieves the best perfor-\\nmance. As mentioned in the introduction, the design of GPEFT\\naims to provide a high-quality, lightweight framework suitable for\\nvarious applications on different text-rich graphs. Considering the\\nreduced parameter storage requirements and the training time, we\\nbelieve GPEFT holds significant potential for industrial-scale ap-\\nplications, including recommendation systems, ranking tasks, and\\netc.\\nRQ5: What is the parameter sensitivity of GPEFT ?\\nThere are several important hyper-parameters in GPEFT : (1)\\nLORA rank ùëü, it affects the number of trainable parameters (2)\\nnumber of hops ùëòin GNN prompt encoder, it affects the amount of\\nstructural information used in the training of GPEFT . In Figure 4,\\nwe observe that varying the rank of LoRA matrices from 4 to 32 does\\nnot significantly affect the performance. This observation aligns\\nwith findings from other studies, which suggest that optimizing for\\na single task requires only minor adjustments to the pre-trained\\nLLM weights. Similarly, varying number of hops in the GNN prompt\\nencoder has a minor effect on performance, until the point where\\nmessage passing begins to aggregate more noisy neighbors than\\nuseful ones (e.g., at 4 hops, as shown in Figure 4b).\\n6 CONCLUSION AND FUTURE WORK\\nThis paper proposes GPEFT to harness LLMs for representation\\nlearning on text-rich graphs. Compared with existing work of ap-\\nplying LLM on graph structure data, our proposed method is the\\nfirst one that generates embeddings from the LLM and therefore can\\nbe applied on numerous industrial applications. More importantly,\\n2https://huggingface.co/tiiuae/falcon-7b', metadata={'source': './prof_1/2404.18271v1.pdf', 'page': 7})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is a GNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60a112-e7d5-4b8f-a037-57a9d5f0c8ee",
   "metadata": {},
   "source": [
    "### Llama 2 with T5 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9179e27d-adfd-4be6-92d4-d6f589fb7bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings = T5Embeddings(\"MBZUAI/LaMini-T5-738M\")\n",
    "    \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5ed92e-3557-4a06-a0ab-293d44c5f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper proposes a novel approach called Graph Prompt Encoder and Fine-tuned LM (GPEFT) that leverages large language models (LLMs) for representation learning on text-rich graphs. The key innovation of GPEFT is the use of a graph neural network (GNN) to encode structural information from neighboring nodes into a graph prompt, which is then fine-tuned with an LLM to learn high-quality representations for downstream tasks.\n",
      "\n",
      "The proposed approach consists of two main components:\n",
      "\n",
      "1. Graph Prompt Encoder (GPE): This component is responsible for encoding the structural information from the input graph into a compact representation, called the graph prompt. The GPE uses a GNN to model the relationships between nodes in the graph and generates a set of embeddings that capture the structural information.\n",
      "2. Fine-tuned LLM (FLLM): This component is responsible for fine-tuning the LLM with the generated graph prompt to learn high-quality representations for downstream tasks. The FLLM uses the pre-trained LLM as a starting point and adapts it to the specific task at hand through a process of fine-tuning.\n",
      "\n",
      "The paper evaluates the performance of GPEFT on several benchmark datasets and compares it with existing approaches that apply LLMs directly to graph structure data. The results show that GPEFT outperforms these existing approaches, demonstrating its effectiveness in leveraging LLMs for representation learning on text-rich graphs.\n",
      "\n",
      "Future work could focus on exploring other architectures and techniques for incorporating structural information into the prompt encoder, such as using attention mechanisms to weight the importance of different nodes or edges in the graph, or developing more sophisticated GNN models that can capture complex patterns in the data. Additionally, there is potential to apply GPEFT to other domains beyond text-rich graphs, such as images or videos, where the structural information can be used to improve the performance of LLMs in downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"LLAMA 2 7B with T5 embeddings\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f4ab7-16c8-4ed2-a671-8e0781a4038f",
   "metadata": {},
   "source": [
    "### Llama 2 with llama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1940411-70c1-4b87-a84c-0e3119b8119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")\n",
    "\n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cb92b-e20c-48b0-a656-d2e4d7d633c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"LLAMA 2 7b with llama embeddings\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfac1c-ea26-4a56-9a8a-319fa069bba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2cace-aef4-4197-a202-b1e521a8e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check t5 instructions finetuning format\n",
    "# GPT-NeoX 1.3\n",
    "# Sliding windows\n",
    "\n",
    "# Shorten retrieve content\n",
    "# Finetune to extend context\n",
    "# Try different techniques for FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c99d7-1bec-494b-99b2-b66a7bc973e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
